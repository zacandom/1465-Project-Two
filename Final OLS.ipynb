{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3d6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach\\AppData\\Local\\Temp\\ipykernel_24904\\2363200274.py:4: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,651,652,653,654,655,656,657,658,659,660,672,683,697,698,699,705,706,707) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"CesDataClean.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CesDataFinal.csv created successfully.\n",
      "Rows: 1368  Columns: 37\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original data\n",
    "df = pd.read_csv(\"CesDataClean.csv\")\n",
    "\n",
    "# Dictionary of variables to extract and rename\n",
    "rename_vars = {\n",
    "    \"treat3\": \"Second_Moment_Treatment\",\n",
    "    \"dedu2\": \"Highschool_Educated\",\n",
    "    \"dedu3\": \"Tertiary_Educated\",\n",
    "\n",
    "    \"cnt1\": \"Belgian\",\n",
    "    \"cnt2\": \"Danish\",\n",
    "    \"cnt3\": \"Spanish\",\n",
    "    \"cnt4\": \"French\",\n",
    "    \"cnt5\": \"Italian\",\n",
    "    \"cnt6\": \"Dutch\",\n",
    "\n",
    "    \"egrea_imean_pt\": \"First_Moment_Expectation_Post\",\n",
    "    \"age\": \"Age\",\n",
    "    \"k1010_3\": \"Growth_Uncertainty_Probability\",\n",
    "    \"hhsize\": \"Household_Size\",\n",
    "    \"male\": \"Male\",\n",
    "    \"lhhnetinc\": \"Log_Household_Net_Income\",\n",
    "    \n",
    "    \"pr2010_Non-probabilistic\": \"Planned_Spending_Consumer_Goods\", \n",
    "    \"pr2110_CAWI\": \"Planned_Spending_Durable_Goods\",\n",
    "\n",
    "    \"egrea_imean_bt\": \"First_Moment_Expectation_Prior\",\n",
    "    \"egrea_istd_bt\": \"Second_Moment_Prior\",\n",
    "    \"egrea_istd_pt\": \"Second_Moment_Post\",\n",
    "\n",
    "    \"treat1\": \"Control_Group\",\n",
    "\n",
    "    \"cons_tot_wave10\": \"Total_Consumption_Oct_2020\",\n",
    "    \"cons_tot_wave13\": \"Total_Consumption_Jan_2021\",\n",
    "    \"cons_tot_wave7\": \"Total_Consumption_Prior\",\n",
    "    \n",
    "    \"cons_dbt_wave10\": \"Total_Debt_Oct_2020\",\n",
    "    \"cons_dbt_wave13\": \"Total_Debt_Jan_2021\",\n",
    "    \"cons_dbt_wave7\": \"Total_Debt_Prior\",\n",
    "    \n",
    "    \"wgt_wave10\": \"Weight_Oct\",\n",
    "    \"wgt_wave13\": \"Weight_Jan\", \n",
    "    \"wgt_wave7\": \"Weight_Prior\", \n",
    "\n",
    "    \"liquid_wave10\": \"Liquidity_Oct\",\n",
    "    \"liquid_wave13\": \"Liquidity_Jan\", \n",
    "    \"liquid_wave7\": \"Liquidity_Prior\", \n",
    "    \n",
    "    \"sh_sav_wave9\": \"Savings_Prior\",\n",
    "    \"sh_stock_wave9\": \"Stocks_Prior\",\n",
    "    \"sh_mutf_wave9\": \"Mutual_Funds_Prior\",\n",
    "\n",
    "    \"sh_bond_wave9\": \"Bonds_Prior\"\n",
    "}\n",
    "\n",
    "# Extract selected variables\n",
    "df_sel = df[list(rename_vars.keys())]\n",
    "\n",
    "# Keep only observations where treat3 == 1 OR treat1 == 1\n",
    "df_sel = df_sel[(df_sel[\"treat3\"] == 1) | (df_sel[\"treat1\"] == 1)]\n",
    "\n",
    "# Rename variables\n",
    "df_final = df_sel.rename(columns=rename_vars)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_final.to_csv(\"CesDataFinal.csv\", index=False)\n",
    "\n",
    "print(\"CesDataFinal.csv created successfully.\")\n",
    "print(\"Rows:\", df_final.shape[0], \" Columns:\", df_final.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57b8497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach\\AppData\\Local\\Temp\\ipykernel_24904\\1839512890.py:7: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,651,652,653,654,655,656,657,658,659,660,672,683,697,698,699,705,706,707) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"CesDataClean.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CesDataTable2.csv created successfully.\n",
      "Rows: 3309  Columns: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) Load principal dataset\n",
    "# ------------------------------------------------\n",
    "df = pd.read_csv(\"CesDataClean.csv\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) Select variables needed for Table 2\n",
    "#    (expectations, treatments, countries, weight)\n",
    "# ------------------------------------------------\n",
    "needed_vars = [\n",
    "    # ID\n",
    "    \"hid\",\n",
    "\n",
    "    # Expectations for EA GDP (no wave suffix: these are the RCT expectations)\n",
    "    \"egrea_imean_bt\",   # prior mean\n",
    "    \"egrea_imean_pt\",   # posterior mean\n",
    "    \"egrea_istd_bt\",    # prior std (uncertainty)\n",
    "    \"egrea_istd_pt\",    # posterior std (uncertainty)\n",
    "\n",
    "    # Treatment arms\n",
    "    \"treat1\",\n",
    "    \"treat2\",\n",
    "    \"treat3\",\n",
    "    \"treat4\",\n",
    "    \"treat5\",\n",
    "\n",
    "    # Country dummies\n",
    "    \"cnt1\",\n",
    "    \"cnt2\",\n",
    "    \"cnt3\",\n",
    "    \"cnt4\",\n",
    "    \"cnt5\",\n",
    "    \"cnt6\",\n",
    "\n",
    "    # Wave-9 weight for expectations\n",
    "    \"wgt_wave9\",\n",
    "\n",
    "    # (Optional, useful later if you want to drop speeders)\n",
    "    \"flag_speeder\"\n",
    "]\n",
    "\n",
    "existing = [v for v in needed_vars if v in df.columns]\n",
    "missing = [v for v in needed_vars if v not in df.columns]\n",
    "\n",
    "if missing:\n",
    "    print(\"Warning: missing expected variables:\", missing)\n",
    "\n",
    "df_sub = df[existing].copy()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Rename to match your previous conventions\n",
    "# ------------------------------------------------\n",
    "rename_map = {\n",
    "    # IDs\n",
    "    \"hid\": \"HH_ID\",\n",
    "\n",
    "    # Expectations: mean\n",
    "    \"egrea_imean_bt\": \"First_Moment_Expectation_Prior\",\n",
    "    \"egrea_imean_pt\": \"First_Moment_Expectation_Post\",\n",
    "\n",
    "    # Expectations: second moment (uncertainty)\n",
    "    \"egrea_istd_bt\":  \"Second_Moment_Prior\",\n",
    "    \"egrea_istd_pt\":  \"Second_Moment_Post\",\n",
    "\n",
    "    # Treatment arms (align with your CesDataFinal naming)\n",
    "    \"treat1\": \"Control_Group\",\n",
    "    \"treat2\": \"First_Moment_Treatment\",\n",
    "    \"treat3\": \"Second_Moment_Treatment\",\n",
    "    \"treat4\": \"Combined_Moments_Treatment\",\n",
    "    \"treat5\": \"Alternative_Treatment\",\n",
    "\n",
    "    # Countries\n",
    "    \"cnt1\": \"Belgian\",\n",
    "    \"cnt2\": \"Danish\",\n",
    "    \"cnt3\": \"Spanish\",\n",
    "    \"cnt4\": \"French\",\n",
    "    \"cnt5\": \"Italian\",\n",
    "    \"cnt6\": \"Dutch\",\n",
    "\n",
    "    # Weight\n",
    "    \"wgt_wave9\": \"Weight_Expectations\",\n",
    "\n",
    "    # Optional\n",
    "    \"flag_speeder\": \"Speeder_Flag\"\n",
    "}\n",
    "\n",
    "df_sub = df_sub.rename(columns=rename_map)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Create log uncertainty variables (like legrea_istd_* in Stata)\n",
    "# ------------------------------------------------\n",
    "if \"Second_Moment_Prior\" in df_sub.columns:\n",
    "    df_sub[\"Log_Second_Moment_Prior\"] = np.log(df_sub[\"Second_Moment_Prior\"])\n",
    "\n",
    "if \"Second_Moment_Post\" in df_sub.columns:\n",
    "    df_sub[\"Log_Second_Moment_Post\"] = np.log(df_sub[\"Second_Moment_Post\"])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Ensure numeric types\n",
    "# ------------------------------------------------\n",
    "numeric_cols = [\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"First_Moment_Expectation_Post\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Second_Moment_Post\",\n",
    "    \"Log_Second_Moment_Prior\",\n",
    "    \"Log_Second_Moment_Post\",\n",
    "    \"Control_Group\",\n",
    "    \"First_Moment_Treatment\",\n",
    "    \"Second_Moment_Treatment\",\n",
    "    \"Combined_Moments_Treatment\",\n",
    "    \"Alternative_Treatment\",\n",
    "    \"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\",\n",
    "    \"Weight_Expectations\",\n",
    "    \"Speeder_Flag\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_sub.columns:\n",
    "        df_sub[col] = pd.to_numeric(df_sub[col], errors=\"coerce\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) (Optional but sensible) drop rows missing key stuff\n",
    "# ------------------------------------------------\n",
    "key_vars = [\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"First_Moment_Expectation_Post\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Second_Moment_Post\",\n",
    "    \"Weight_Expectations\"\n",
    "]\n",
    "\n",
    "df_sub = df_sub.dropna(subset=[c for c in key_vars if c in df_sub.columns])\n",
    "\n",
    "# If you want to drop speeders here, uncomment:\n",
    "# if \"Speeder_Flag\" in df_sub.columns:\n",
    "#     df_sub = df_sub[df_sub[\"Speeder_Flag\"] == 0]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 7) Save Table-2-ready dataset\n",
    "# ------------------------------------------------\n",
    "df_sub.to_csv(\"CesDataTable2.csv\", index=False)\n",
    "\n",
    "print(\"CesDataTable2.csv created successfully.\")\n",
    "print(\"Rows:\", df_sub.shape[0], \" Columns:\", df_sub.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce5ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach\\AppData\\Local\\Temp\\ipykernel_24904\\1881455543.py:7: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,651,652,653,654,655,656,657,658,659,660,672,683,697,698,699,705,706,707) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"CesDataClean.csv\")\n",
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CesDataTable3.csv created successfully.\n",
      "Rows: 3310  Columns: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Load principal CES dataset\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_csv(\"CesDataClean.csv\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2) Variables needed for Table 3 (baseline spec)\n",
    "# ---------------------------------------------------\n",
    "needed_vars = [\n",
    "    # ID\n",
    "    \"hid\",\n",
    "\n",
    "    # Expectations: mean + second moment (EA GDP)\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "\n",
    "    # Treatments (for later IV / robustness if needed)\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\", \"treat5\",\n",
    "\n",
    "    # Demographics / controls\n",
    "    \"dedu2\", \"dedu3\",\n",
    "    \"age\", \"hhsize\",\n",
    "    \"lhhnetinc\",\n",
    "    \"male\",\n",
    "    \"k1010_3\",   # growth uncertainty prob\n",
    "\n",
    "    # Planned spending dummies\n",
    "    \"pr2010_Non-probabilistic\",\n",
    "    \"pr2110_CAWI\",\n",
    "\n",
    "    # Liquidity by wave\n",
    "    \"liquid_wave10\",\n",
    "    \"liquid_wave13\",\n",
    "\n",
    "    # Consumption & debt by wave (Oct & Jan)\n",
    "    \"cons_tot_wave10\", \"cons_dbt_wave10\",\n",
    "    \"cons_tot_wave13\", \"cons_dbt_wave13\",\n",
    "\n",
    "    # (Optional) prior consumption if you ever want it\n",
    "    \"cons_tot_wave7\", \"cons_dbt_wave7\",\n",
    "\n",
    "    # Weights\n",
    "    \"wgt_wave9\",   # expectations wave\n",
    "    \"wgt_wave10\",  # October\n",
    "    \"wgt_wave13\",  # January\n",
    "\n",
    "    # Country dummies\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "]\n",
    "\n",
    "existing = [v for v in needed_vars if v in df.columns]\n",
    "missing = [v for v in needed_vars if v not in df.columns]\n",
    "if missing:\n",
    "    print(\"Warning: missing expected variables:\", missing)\n",
    "\n",
    "df_sub = df[existing].copy()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3) Rename variables to your convention\n",
    "# ---------------------------------------------------\n",
    "rename_map = {\n",
    "    \"hid\": \"HH_ID\",\n",
    "\n",
    "    # Expectations\n",
    "    \"egrea_imean_bt\": \"First_Moment_Expectation_Prior\",\n",
    "    \"egrea_imean_pt\": \"First_Moment_Expectation_Post\",\n",
    "    \"egrea_istd_bt\":  \"Second_Moment_Prior\",\n",
    "    \"egrea_istd_pt\":  \"Second_Moment_Post\",\n",
    "\n",
    "    # Treatments\n",
    "    \"treat1\": \"Control_Group\",\n",
    "    \"treat2\": \"First_Moment_Treatment\",\n",
    "    \"treat3\": \"Second_Moment_Treatment\",\n",
    "    \"treat4\": \"Combined_Moments_Treatment\",\n",
    "    \"treat5\": \"Alternative_Treatment\",\n",
    "\n",
    "    # Demographics / controls\n",
    "    \"dedu2\": \"Highschool_Educated\",\n",
    "    \"dedu3\": \"Tertiary_Educated\",\n",
    "    \"age\": \"Age\",\n",
    "    \"hhsize\": \"Household_Size\",\n",
    "    \"lhhnetinc\": \"Log_Household_Net_Income\",\n",
    "    \"male\": \"Male\",\n",
    "    \"k1010_3\": \"Growth_Uncertainty_Probability\",\n",
    "\n",
    "    \"pr2010_Non-probabilistic\": \"Planned_Spending_Consumer_Goods\",\n",
    "    \"pr2110_CAWI\": \"Planned_Spending_Durable_Goods\",\n",
    "\n",
    "    # Liquidity\n",
    "    \"liquid_wave10\": \"Liquidity_Oct\",\n",
    "    \"liquid_wave13\": \"Liquidity_Jan\",\n",
    "\n",
    "    # Consumption & debt\n",
    "    \"cons_tot_wave10\": \"Total_Consumption_Oct_2020\",\n",
    "    \"cons_dbt_wave10\": \"Total_Debt_Oct_2020\",\n",
    "    \"cons_tot_wave13\": \"Total_Consumption_Jan_2021\",\n",
    "    \"cons_dbt_wave13\": \"Total_Debt_Jan_2021\",\n",
    "    \"cons_tot_wave7\":  \"Total_Consumption_Prior\",\n",
    "    \"cons_dbt_wave7\":  \"Total_Debt_Prior\",\n",
    "\n",
    "    # Weights\n",
    "    \"wgt_wave9\":  \"Weight_Expectations\",\n",
    "    \"wgt_wave10\": \"Weight_Oct\",\n",
    "    \"wgt_wave13\": \"Weight_Jan\",\n",
    "\n",
    "    # Country dummies\n",
    "    \"cnt1\": \"Belgian\",\n",
    "    \"cnt2\": \"Danish\",\n",
    "    \"cnt3\": \"Spanish\",\n",
    "    \"cnt4\": \"French\",\n",
    "    \"cnt5\": \"Italian\",\n",
    "    \"cnt6\": \"Dutch\",\n",
    "}\n",
    "\n",
    "df_sub = df_sub.rename(columns=rename_map)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4) Coerce relevant columns to numeric BEFORE transformations\n",
    "# ---------------------------------------------------\n",
    "numeric_cols = [\n",
    "    # expectations\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"First_Moment_Expectation_Post\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Second_Moment_Post\",\n",
    "\n",
    "    # treatments\n",
    "    \"Control_Group\",\n",
    "    \"First_Moment_Treatment\",\n",
    "    \"Second_Moment_Treatment\",\n",
    "    \"Combined_Moments_Treatment\",\n",
    "    \"Alternative_Treatment\",\n",
    "\n",
    "    # demos / controls\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    \"Age\",\n",
    "    \"Household_Size\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Male\",\n",
    "    \"Growth_Uncertainty_Probability\",\n",
    "    \"Planned_Spending_Consumer_Goods\",\n",
    "    \"Planned_Spending_Durable_Goods\",\n",
    "\n",
    "    # liquidity\n",
    "    \"Liquidity_Oct\",\n",
    "    \"Liquidity_Jan\",\n",
    "\n",
    "    # consumption & debt\n",
    "    \"Total_Consumption_Oct_2020\",\n",
    "    \"Total_Debt_Oct_2020\",\n",
    "    \"Total_Consumption_Jan_2021\",\n",
    "    \"Total_Debt_Jan_2021\",\n",
    "    \"Total_Consumption_Prior\",\n",
    "    \"Total_Debt_Prior\",\n",
    "\n",
    "    # weights\n",
    "    \"Weight_Expectations\",\n",
    "    \"Weight_Oct\",\n",
    "    \"Weight_Jan\",\n",
    "\n",
    "    # countries\n",
    "    \"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_sub.columns:\n",
    "        df_sub[col] = pd.to_numeric(df_sub[col], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5) Create derived variables: Age^2, net & log consumption\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Age squared / 100 (as in Stata)\n",
    "if \"Age\" in df_sub.columns:\n",
    "    df_sub[\"Age_Squared\"] = (df_sub[\"Age\"] ** 2) / 100.0\n",
    "\n",
    "# Net consumption: cons_tot - cons_dbt\n",
    "if all(c in df_sub.columns for c in [\"Total_Consumption_Oct_2020\", \"Total_Debt_Oct_2020\"]):\n",
    "    df_sub[\"Net_Consumption_Oct\"] = (\n",
    "        df_sub[\"Total_Consumption_Oct_2020\"] - df_sub[\"Total_Debt_Oct_2020\"]\n",
    "    )\n",
    "\n",
    "if all(c in df_sub.columns for c in [\"Total_Consumption_Jan_2021\", \"Total_Debt_Jan_2021\"]):\n",
    "    df_sub[\"Net_Consumption_Jan\"] = (\n",
    "        df_sub[\"Total_Consumption_Jan_2021\"] - df_sub[\"Total_Debt_Jan_2021\"]\n",
    "    )\n",
    "\n",
    "if all(c in df_sub.columns for c in [\"Total_Consumption_Prior\", \"Total_Debt_Prior\"]):\n",
    "    df_sub[\"Net_Consumption_Prior\"] = (\n",
    "        df_sub[\"Total_Consumption_Prior\"] - df_sub[\"Total_Debt_Prior\"]\n",
    "    )\n",
    "\n",
    "# Log consumption (like lc in Stata)\n",
    "for net_col, log_col in [\n",
    "    (\"Net_Consumption_Oct\", \"Log_Consumption_Oct\"),\n",
    "    (\"Net_Consumption_Jan\", \"Log_Consumption_Jan\"),\n",
    "    (\"Net_Consumption_Prior\", \"Log_Consumption_Prior\"),\n",
    "]:\n",
    "    if net_col in df_sub.columns:\n",
    "        df_sub[log_col] = np.log(df_sub[net_col])\n",
    "\n",
    "        # Trim extremes: lc < 5 or lc > 10 -> missing\n",
    "        df_sub.loc[df_sub[log_col] < 5, log_col] = np.nan\n",
    "        df_sub.loc[df_sub[log_col] > 10, log_col] = np.nan\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6) Final numeric coercion (just to be safe)\n",
    "# ---------------------------------------------------\n",
    "for col in df_sub.columns:\n",
    "    if col != \"HH_ID\":\n",
    "        df_sub[col] = pd.to_numeric(df_sub[col], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7) Save Table-3-ready dataset\n",
    "# ---------------------------------------------------\n",
    "df_sub.to_csv(\"CesDataTable3.csv\", index=False)\n",
    "\n",
    "print(\"CesDataTable3.csv created successfully.\")\n",
    "print(\"Rows:\", df_sub.shape[0], \" Columns:\", df_sub.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab83dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CesDataFinal.csv updated successfully with new variables:\n",
      "- age2\n",
      "- Net_Consumption_Oct / Net_Consumption_Jan\n",
      "- Log_Consumption_Oct / Log_Consumption_Jan (trimmed and scaled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Load existing dataset\n",
    "# -------------------------------------------------------\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Create age squared (Age is already renamed)\n",
    "# -------------------------------------------------------\n",
    "df[\"age2\"] = (df[\"Age\"] ** 2) / 100.0\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Create net consumption variables\n",
    "#     Net_Consumption_Oct = Total_Consumption_Oct_2020 - Total_Debt_Oct_2020\n",
    "#     Net_Consumption_Jan = Total_Consumption_Jan_2021 - Total_Debt_Jan_2021\n",
    "# -------------------------------------------------------\n",
    "df[\"Net_Consumption_Oct\"] = (\n",
    "    df[\"Total_Consumption_Oct_2020\"] - df[\"Total_Debt_Oct_2020\"]\n",
    ")\n",
    "\n",
    "df[\"Net_Consumption_Jan\"] = (\n",
    "    df[\"Total_Consumption_Jan_2021\"] - df[\"Total_Debt_Jan_2021\"]\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) Log consumption (raw logs)\n",
    "#     This matches: log(cons_tot - cons_dbt)\n",
    "# -------------------------------------------------------\n",
    "df[\"Log_Consumption_Oct\"] = np.log(df[\"Net_Consumption_Oct\"])\n",
    "df[\"Log_Consumption_Jan\"] = np.log(df[\"Net_Consumption_Jan\"])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) Apply trimming EXACTLY like the Stata code\n",
    "#     replace lc = . if lc < 5 or lc > 10\n",
    "# -------------------------------------------------------\n",
    "for col in [\"Log_Consumption_Oct\", \"Log_Consumption_Jan\"]:\n",
    "    df.loc[df[col] < 5, col] = np.nan\n",
    "    df.loc[df[col] > 10, col] = np.nan\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Multiply by 100 (Coibion et al. scaling)\n",
    "# -------------------------------------------------------\n",
    "df[\"Log_Consumption_Oct\"] = df[\"Log_Consumption_Oct\"] * 100\n",
    "df[\"Log_Consumption_Jan\"] = df[\"Log_Consumption_Jan\"] * 100\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Save back to SAME FILE (no new dataset created)\n",
    "# -------------------------------------------------------\n",
    "df.to_csv(\"CesDataFinal.csv\", index=False)\n",
    "\n",
    "print(\"CesDataFinal.csv updated successfully with new variables:\")\n",
    "print(\"- age2\")\n",
    "print(\"- Net_Consumption_Oct / Net_Consumption_Jan\")\n",
    "print(\"- Log_Consumption_Oct / Log_Consumption_Jan (trimmed and scaled)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0af5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH_ID\n",
      "First_Moment_Expectation_Prior\n",
      "First_Moment_Expectation_Post\n",
      "Second_Moment_Prior\n",
      "Second_Moment_Post\n",
      "Control_Group\n",
      "First_Moment_Treatment\n",
      "Second_Moment_Treatment\n",
      "Combined_Moments_Treatment\n",
      "Alternative_Treatment\n",
      "Highschool_Educated\n",
      "Tertiary_Educated\n",
      "Age\n",
      "Household_Size\n",
      "Log_Household_Net_Income\n",
      "Male\n",
      "Growth_Uncertainty_Probability\n",
      "Planned_Spending_Consumer_Goods\n",
      "Planned_Spending_Durable_Goods\n",
      "Liquidity_Oct\n",
      "Liquidity_Jan\n",
      "Total_Consumption_Oct_2020\n",
      "Total_Debt_Oct_2020\n",
      "Total_Consumption_Jan_2021\n",
      "Total_Debt_Jan_2021\n",
      "Total_Consumption_Prior\n",
      "Total_Debt_Prior\n",
      "Weight_Expectations\n",
      "Weight_Oct\n",
      "Weight_Jan\n",
      "Belgian\n",
      "Danish\n",
      "Spanish\n",
      "French\n",
      "Italian\n",
      "Dutch\n",
      "Age_Squared\n",
      "Net_Consumption_Oct\n",
      "Net_Consumption_Jan\n",
      "Net_Consumption_Prior\n",
      "Log_Consumption_Oct\n",
      "Log_Consumption_Jan\n",
      "Log_Consumption_Prior\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"CesDataTable3.csv\")\n",
    "\n",
    "# Print all variable names\n",
    "for col in df.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "517db51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Log consumption columns cleaned and stored as numeric floats.\n",
      "Log_Consumption_Oct    float64\n",
      "Log_Consumption_Jan    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# List of columns to clean\n",
    "cols_to_fix = [\"Log_Consumption_Oct\", \"Log_Consumption_Jan\"]\n",
    "\n",
    "for col in cols_to_fix:\n",
    "    if col in df.columns:\n",
    "        # Remove quotes, apostrophes, whitespace\n",
    "        df[col] = df[col].astype(str).str.replace(\"'\", \"\", regex=False).str.strip()\n",
    "\n",
    "        # Convert to numeric\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Save back to same file\n",
    "df.to_csv(\"CesDataFinal.csv\", index=False)\n",
    "\n",
    "print(\"✔ Log consumption columns cleaned and stored as numeric floats.\")\n",
    "print(df[cols_to_fix].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40558912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         spec   obs  beta_treat_rounded  se_treat_rounded\n",
      "0              Spec 1: T only  1313             85.7958           51.8819\n",
      "1    Spec 2: T + pre-controls  1313             94.5284           45.5745\n",
      "2  Spec 3: + post expectation  1313             95.9094           45.5738\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0) Load data + basic sample restrictions\n",
    "# --------------------------------------------------\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# Keep only treatment/control observations (in case not already filtered)\n",
    "df = df[(df[\"Second_Moment_Treatment\"] == 1) | (df[\"Control_Group\"] == 1)]\n",
    "\n",
    "# Drop Jan consumption outliers\n",
    "df = df[df[\"Total_Consumption_Jan_2021\"] <= 5000]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Define variables\n",
    "# --------------------------------------------------\n",
    "Y = \"Total_Consumption_Jan_2021\"\n",
    "treat = \"Second_Moment_Treatment\"\n",
    "mediator = \"Second_Moment_Post\"   # post expectation (info channel)\n",
    "\n",
    "# Pre-treatment controls (NO post variables, NO Oct consumption)\n",
    "# all of these should be in CesDataFinal from earlier step\n",
    "pre_treatment_controls = [\n",
    "    \"Age\",\n",
    "    \"Household_Size\",\n",
    "    \"Male\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    # nationality dummies, with Belgian as baseline (so we EXCLUDE Belgian)\n",
    "    \"Danish\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"Italian\",\n",
    "    \"Dutch\",\n",
    "    # baseline portfolio and prior expectations\n",
    "    \"Savings_Prior\",\n",
    "    \"Stocks_Prior\",\n",
    "    \"Mutual_Funds_Prior\",\n",
    "    \"Bonds_Prior\",\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Total_Consumption_Prior\"\n",
    "]\n",
    "\n",
    "# We explicitly do NOT use \"Second_Moment_Post\" anywhere\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Ensure everything is numeric where needed\n",
    "# --------------------------------------------------\n",
    "all_needed = [Y, treat] + pre_treatment_controls + [mediator]\n",
    "\n",
    "# Some columns might not exist (e.g. if mediator not created yet) -> guard\n",
    "all_needed_existing = [col for col in all_needed if col in df.columns]\n",
    "\n",
    "df[all_needed_existing] = df[all_needed_existing].apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Helper to run a spec and return coef + se\n",
    "# --------------------------------------------------\n",
    "def run_spec(x_cols, spec_name):\n",
    "    X = df[x_cols].copy()\n",
    "    y = df[Y].copy()\n",
    "\n",
    "    # Drop rows with NA in any regression variable\n",
    "    mask = X.notna().all(axis=1) & y.notna()\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X).fit(cov_type=\"HC1\")\n",
    "\n",
    "    coef = model.params[treat]\n",
    "    se = model.bse[treat]\n",
    "\n",
    "    return {\n",
    "        \"spec\": spec_name,\n",
    "        \"obs\": int(X.shape[0]),\n",
    "        \"beta_treat\": coef,\n",
    "        \"se_treat\": se\n",
    "    }\n",
    "\n",
    "results = []\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Spec 1: Reduced-form, treatment only\n",
    "# --------------------------------------------------\n",
    "results.append(\n",
    "    run_spec([treat], \"Spec 1: T only\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Spec 2: Treatment + pre-treatment controls\n",
    "# --------------------------------------------------\n",
    "results.append(\n",
    "    run_spec([treat] + pre_treatment_controls, \"Spec 2: T + pre-controls\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Spec 3: Treatment + pre-treatment controls + post expectation (mediator)\n",
    "# --------------------------------------------------\n",
    "if mediator in df.columns:\n",
    "    results.append(\n",
    "        run_spec([treat] + pre_treatment_controls + [mediator],\n",
    "                 \"Spec 3: + post expectation\")\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: mediator variable\", mediator, \"not found in data; Spec 3 skipped.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Show results in a small table\n",
    "# --------------------------------------------------\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df[\"beta_treat_rounded\"] = res_df[\"beta_treat\"].round(4)\n",
    "res_df[\"se_treat_rounded\"] = res_df[\"se_treat\"].round(4)\n",
    "\n",
    "print(res_df[[\"spec\", \"obs\", \"beta_treat_rounded\", \"se_treat_rounded\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9de1bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs used: 1313\n",
      "Beta (Second_Moment_Treatment): 101.7736\n",
      "Robust SE: 48.0085\n",
      "t-stat: 2.12\n",
      "                                OLS Regression Results                                \n",
      "======================================================================================\n",
      "Dep. Variable:     Total_Consumption_Jan_2021   R-squared:                       0.162\n",
      "Model:                                    OLS   Adj. R-squared:                  0.151\n",
      "Method:                         Least Squares   F-statistic:                     13.56\n",
      "Date:                        Thu, 13 Nov 2025   Prob (F-statistic):           5.66e-36\n",
      "Time:                                19:06:28   Log-Likelihood:                -10736.\n",
      "No. Observations:                        1313   AIC:                         2.151e+04\n",
      "Df Residuals:                            1295   BIC:                         2.160e+04\n",
      "Df Model:                                  17                                         \n",
      "Covariance Type:                          HC1                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                            -81.5428    283.582     -0.288      0.774    -637.353     474.268\n",
      "Second_Moment_Treatment          101.7736     48.009      2.120      0.034       7.679     195.868\n",
      "Age                                4.0268      1.760      2.288      0.022       0.578       7.476\n",
      "Household_Size                   181.1705     19.785      9.157      0.000     142.393     219.948\n",
      "Male                              98.3325     49.979      1.967      0.049       0.376     196.289\n",
      "Log_Household_Net_Income         129.8620     24.423      5.317      0.000      81.994     177.730\n",
      "Highschool_Educated              -57.3902     86.615     -0.663      0.508    -227.153     112.372\n",
      "Tertiary_Educated                114.0159     82.139      1.388      0.165     -46.974     275.006\n",
      "Danish                           367.6825     70.511      5.215      0.000     229.484     505.881\n",
      "Spanish                          -14.4381     70.636     -0.204      0.838    -152.882     124.006\n",
      "French                           233.8221     77.436      3.020      0.003      82.051     385.593\n",
      "Dutch                            172.4910     81.732      2.110      0.035      12.299     332.683\n",
      "Savings_Prior                   -422.2224     81.283     -5.194      0.000    -581.534    -262.911\n",
      "Stocks_Prior                    -550.0614    161.962     -3.396      0.001    -867.500    -232.622\n",
      "Mutual_Funds_Prior               -99.6633    167.192     -0.596      0.551    -427.353     228.027\n",
      "Bonds_Prior                     -422.5731    288.956     -1.462      0.144    -988.916     143.770\n",
      "First_Moment_Expectation_Prior    -1.5303      2.838     -0.539      0.590      -7.093       4.032\n",
      "Second_Moment_Prior               -3.7448     12.179     -0.307      0.758     -27.615      20.125\n",
      "==============================================================================\n",
      "Omnibus:                       81.265   Durbin-Watson:                   1.940\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               98.552\n",
      "Skew:                           0.597   Prob(JB):                     3.98e-22\n",
      "Kurtosis:                       3.613   Cond. No.                         608.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1) Load data\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# 2) Keep only treatment/control sample (if not already restricted)\n",
    "df = df[(df[\"Second_Moment_Treatment\"] == 1) | (df[\"Control_Group\"] == 1)]\n",
    "\n",
    "# 3) Drop outliers in Jan consumption\n",
    "df = df[df[\"Total_Consumption_Jan_2021\"] <= 5000]\n",
    "\n",
    "# 4) Define core variables\n",
    "Y = \"Total_Consumption_Jan_2021\"\n",
    "treat = \"Second_Moment_Treatment\"\n",
    "\n",
    "# 5) Pre-treatment controls (no post variables, no October cons, no mediator)\n",
    "pre_treatment_controls = [\n",
    "    \"Age\",\n",
    "    \"Household_Size\",\n",
    "    \"Male\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    # nationality dummies with Italy as baseline (so exclude Italian and Belgian)\n",
    "    \"Danish\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"Dutch\",\n",
    "    # baseline portfolio and prior expectations\n",
    "    \"Savings_Prior\",\n",
    "    \"Stocks_Prior\",\n",
    "    \"Mutual_Funds_Prior\",\n",
    "    \"Bonds_Prior\",\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\"\n",
    "]\n",
    "\n",
    "# 6) Make sure the columns exist (in case some names differ)\n",
    "pre_treatment_controls = [c for c in pre_treatment_controls if c in df.columns]\n",
    "\n",
    "# 7) Build X and y\n",
    "X = df[[treat] + pre_treatment_controls].copy()\n",
    "y = df[Y].copy()\n",
    "\n",
    "# 8) Convert to numeric and drop missing\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y = pd.to_numeric(y, errors=\"coerce\")\n",
    "\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 9) Add intercept and run OLS with robust SEs\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit(cov_type=\"HC1\")\n",
    "\n",
    "# 10) Extract treatment effect\n",
    "coef = model.params[treat]\n",
    "se = model.bse[treat]\n",
    "t_stat = coef / se\n",
    "\n",
    "print(\"Obs used:\", X.shape[0])\n",
    "print(\"Beta (Second_Moment_Treatment):\", round(coef, 4))\n",
    "print(\"Robust SE:\", round(se, 4))\n",
    "print(\"t-stat:\", round(t_stat, 3))\n",
    "\n",
    "# If you want the full table:\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53b8164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression observations used: 1318\n",
      "Coefficient on Second_Moment_Treatment: 37.0424\n",
      "Robust Std. Error: 46.0097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1) Load dataset\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# 2) Remove consumption outliers above 5000\n",
    "df = df[df[\"Total_Consumption_Oct_2020\"] <= 5000]\n",
    "\n",
    "# 3) Variable names\n",
    "Y = \"Total_Consumption_Oct_2020\"\n",
    "treat = \"Second_Moment_Treatment\"\n",
    "\n",
    "# 4) Variables to exclude from the regression\n",
    "exclude = [\n",
    "    \"Total_Consumption_Jan_2021\",      # dependent variable\n",
    "    \"Total_Consumption_Oct_2020\",      # omit this control\n",
    "    \"Second_Moment_Post\",\n",
    "    \"First_Moment_Expectation_Post\", \n",
    "    \"Control_Group\"]\n",
    "\n",
    "# 5) Nationality dummies (Belgium as baseline)\n",
    "nationalities = [\"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"]\n",
    "exclude.append(\"Belgian\")  # baseline category\n",
    "\n",
    "# 6) Build control list\n",
    "controls = [\n",
    "    col for col in df.columns\n",
    "    if col not in exclude + [treat]\n",
    "]\n",
    "\n",
    "# 7) Construct the design matrix\n",
    "X = df[[treat] + controls].copy()\n",
    "y = df[Y].copy()\n",
    "\n",
    "# 8) Convert all to numeric (fix dtype errors)\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y = pd.to_numeric(y, errors=\"coerce\")\n",
    "\n",
    "# 9) Drop rows with NA values after conversion\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 10) Add intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# 11) Fit OLS with robust standard errors\n",
    "model = sm.OLS(y, X).fit(cov_type=\"HC1\")\n",
    "\n",
    "# 12) Extract coefficient and SE for treatment variable\n",
    "coef = model.params[treat]\n",
    "se = model.bse[treat]\n",
    "\n",
    "print(\"Regression observations used:\", X.shape[0])\n",
    "print(\"Coefficient on Second_Moment_Treatment:\", round(coef, 4))\n",
    "print(\"Robust Std. Error:\", round(se, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0c71cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression observations used: 1318\n",
      "Coefficient on Second_Moment_Treatment: 52.5573\n",
      "Robust Std. Error: 48.3711\n",
      "                                OLS Regression Results                                \n",
      "======================================================================================\n",
      "Dep. Variable:     Total_Consumption_Oct_2020   R-squared:                       0.177\n",
      "Model:                                    OLS   Adj. R-squared:                  0.166\n",
      "Method:                         Least Squares   F-statistic:                     14.71\n",
      "Date:                        Fri, 14 Nov 2025   Prob (F-statistic):           2.42e-39\n",
      "Time:                                15:04:07   Log-Likelihood:                -10794.\n",
      "No. Observations:                        1318   AIC:                         2.162e+04\n",
      "Df Residuals:                            1300   BIC:                         2.172e+04\n",
      "Df Model:                                  17                                         \n",
      "Covariance Type:                          HC1                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                            -54.0960    340.278     -0.159      0.874    -721.030     612.838\n",
      "Second_Moment_Treatment           52.5573     48.371      1.087      0.277     -42.248     147.363\n",
      "Age                                4.9421      1.833      2.696      0.007       1.349       8.535\n",
      "Household_Size                   196.9712     19.956      9.870      0.000     157.857     236.085\n",
      "Male                              80.9983     50.099      1.617      0.106     -17.193     179.190\n",
      "Log_Household_Net_Income         122.2369     29.597      4.130      0.000      64.228     180.246\n",
      "Highschool_Educated               39.7150     84.700      0.469      0.639    -126.294     205.724\n",
      "Tertiary_Educated                247.1727     77.996      3.169      0.002      94.303     400.042\n",
      "Danish                           379.2017     72.511      5.230      0.000     237.083     521.320\n",
      "Spanish                          -82.7207     72.508     -1.141      0.254    -224.834      59.393\n",
      "French                           219.5463     76.829      2.858      0.004      68.964     370.129\n",
      "Dutch                             93.0528     77.924      1.194      0.232     -59.675     245.781\n",
      "Savings_Prior                   -415.0826     82.602     -5.025      0.000    -576.979    -253.186\n",
      "Stocks_Prior                    -469.5329    166.009     -2.828      0.005    -794.905    -144.161\n",
      "Mutual_Funds_Prior              -148.6257    173.696     -0.856      0.392    -489.063     191.812\n",
      "Bonds_Prior                     -993.9430    217.230     -4.576      0.000   -1419.707    -568.179\n",
      "First_Moment_Expectation_Prior    -4.2530      2.634     -1.615      0.106      -9.415       0.909\n",
      "Second_Moment_Prior              -16.2324     12.997     -1.249      0.212     -41.707       9.242\n",
      "==============================================================================\n",
      "Omnibus:                      101.909   Durbin-Watson:                   1.892\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              128.104\n",
      "Skew:                           0.686   Prob(JB):                     1.52e-28\n",
      "Kurtosis:                       3.671   Cond. No.                         614.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1) Load dataset\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# 2) Keep only treatment/control observations\n",
    "df = df[(df[\"Second_Moment_Treatment\"] == 1) | (df[\"Control_Group\"] == 1)]\n",
    "\n",
    "# 3) Remove consumption outliers above 5000 for October\n",
    "df = df[df[\"Total_Consumption_Oct_2020\"] <= 5000]\n",
    "\n",
    "# 4) Variable names\n",
    "Y = \"Total_Consumption_Oct_2020\"\n",
    "treat = \"Second_Moment_Treatment\"\n",
    "\n",
    "# 5) Pre-treatment controls (NO post variables, NO mediators, NO Control_Group)\n",
    "#    Italy is the nationality baseline (so we do NOT include \"Italian\" or \"Belgian\")\n",
    "pre_treatment_controls = [\n",
    "    \"Age\",\n",
    "    \"Household_Size\",\n",
    "    \"Male\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    # nationality dummies (Italy baseline -> omit \"Italian\")\n",
    "    \"Danish\",\n",
    "    \"Spanish\",\n",
    "    \"French\",\n",
    "    \"Dutch\",\n",
    "    # baseline portfolio and prior expectations\n",
    "    \"Savings_Prior\",\n",
    "    \"Stocks_Prior\",\n",
    "    \"Mutual_Funds_Prior\",\n",
    "    \"Bonds_Prior\",\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\"\n",
    "]\n",
    "\n",
    "# Keep only controls that actually exist in the data (defensive)\n",
    "pre_treatment_controls = [c for c in pre_treatment_controls if c in df.columns]\n",
    "\n",
    "# 6) Build design matrix\n",
    "X = df[[treat] + pre_treatment_controls].copy()\n",
    "y = df[Y].copy()\n",
    "\n",
    "# 7) Convert to numeric and drop missing\n",
    "X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "y = pd.to_numeric(y, errors=\"coerce\")\n",
    "\n",
    "mask = X.notna().all(axis=1) & y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# 8) Add intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# 9) Fit OLS with robust standard errors\n",
    "model = sm.OLS(y, X).fit(cov_type=\"HC1\")\n",
    "\n",
    "# 10) Extract coefficient and SE for treatment\n",
    "coef = model.params[treat]\n",
    "se = model.bse[treat]\n",
    "\n",
    "print(\"Regression observations used:\", X.shape[0])\n",
    "print(\"Coefficient on Second_Moment_Treatment:\", round(coef, 4))\n",
    "print(\"Robust Std. Error:\", round(se, 4))\n",
    "\n",
    "# Optional: full summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b1e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================\n",
      "Table-2 style regression for mean expectations\n",
      "Dependent variable: First_Moment_Expectation_Post\n",
      "==============================================\n",
      "                                  WLS Regression Results                                 \n",
      "=========================================================================================\n",
      "Dep. Variable:     First_Moment_Expectation_Post   R-squared:                       0.454\n",
      "Model:                                       WLS   Adj. R-squared:                  0.452\n",
      "Method:                            Least Squares   F-statistic:                     154.6\n",
      "Date:                           Sat, 15 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                                   15:51:31   Log-Likelihood:                -9474.0\n",
      "No. Observations:                           3309   AIC:                         1.898e+04\n",
      "Df Residuals:                               3294   BIC:                         1.907e+04\n",
      "Df Model:                                     14                                         \n",
      "Covariance Type:                             HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.4240      0.150      2.826      0.005       0.130       0.718\n",
      "Prior          0.7087      0.024     29.530      0.000       0.662       0.756\n",
      "Prior_T2      -0.4675      0.032    -14.778      0.000      -0.529      -0.405\n",
      "Prior_T3      -0.2245      0.036     -6.297      0.000      -0.294      -0.155\n",
      "Prior_T4      -0.5376      0.031    -17.622      0.000      -0.597      -0.478\n",
      "Prior_T5      -0.2124      0.034     -6.201      0.000      -0.280      -0.145\n",
      "Treat2A        2.0518      0.170     12.094      0.000       1.719       2.384\n",
      "Treat3A        0.8495      0.178      4.764      0.000       0.500       1.199\n",
      "Treat4A        2.5540      0.170     14.999      0.000       2.220       2.888\n",
      "Treat5A        0.7219      0.188      3.838      0.000       0.353       1.091\n",
      "Belgian       -0.3596      0.215     -1.671      0.095      -0.781       0.062\n",
      "Spanish        0.2077      0.152      1.365      0.172      -0.091       0.506\n",
      "French        -0.2825      0.149     -1.901      0.057      -0.574       0.009\n",
      "Italian        0.3514      0.144      2.443      0.015       0.069       0.633\n",
      "Dutch         -0.9513      0.195     -4.875      0.000      -1.334      -0.569\n",
      "==============================================================================\n",
      "Omnibus:                      455.342   Durbin-Watson:                   1.938\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              951.744\n",
      "Skew:                           0.832   Prob(JB):                    2.15e-207\n",
      "Kurtosis:                       5.033   Cond. No.                         48.0\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "mean expectations – Treat2A:\n",
      "  Coefficient: 2.052\n",
      "  Std. Error: 0.170\n",
      "  t-stat: 12.09\n",
      "\n",
      "mean expectations – Prior_T2:\n",
      "  Coefficient: -0.467\n",
      "  Std. Error: 0.032\n",
      "  t-stat: -14.78\n",
      "\n",
      "==============================================\n",
      "Table-2 style regression for uncertainty\n",
      "Dependent variable: Second_Moment_Post\n",
      "==============================================\n",
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     Second_Moment_Post   R-squared:                       0.258\n",
      "Model:                            WLS   Adj. R-squared:                  0.254\n",
      "Method:                 Least Squares   F-statistic:                     53.54\n",
      "Date:                Sat, 15 Nov 2025   Prob (F-statistic):          2.61e-135\n",
      "Time:                        15:51:31   Log-Likelihood:                -6312.6\n",
      "No. Observations:                3309   AIC:                         1.266e+04\n",
      "Df Residuals:                    3294   BIC:                         1.275e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9292      0.091     10.178      0.000       0.750       1.108\n",
      "Prior          0.8593      0.053     16.298      0.000       0.756       0.963\n",
      "Prior_T2      -0.4947      0.067     -7.366      0.000      -0.626      -0.363\n",
      "Prior_T3      -0.5478      0.064     -8.602      0.000      -0.673      -0.423\n",
      "Prior_T4      -0.5521      0.066     -8.364      0.000      -0.682      -0.423\n",
      "Prior_T5      -0.4072      0.063     -6.448      0.000      -0.531      -0.283\n",
      "Treat2A        0.4473      0.109      4.096      0.000       0.233       0.661\n",
      "Treat3A        0.6215      0.106      5.862      0.000       0.414       0.829\n",
      "Treat4A        0.3200      0.106      3.012      0.003       0.112       0.528\n",
      "Treat5A        0.5449      0.108      5.059      0.000       0.334       0.756\n",
      "Belgian        0.0525      0.095      0.552      0.581      -0.134       0.239\n",
      "Spanish       -0.1294      0.062     -2.100      0.036      -0.250      -0.009\n",
      "French        -0.1352      0.065     -2.075      0.038      -0.263      -0.007\n",
      "Italian       -0.1128      0.061     -1.858      0.063      -0.232       0.006\n",
      "Dutch         -0.0413      0.072     -0.575      0.565      -0.182       0.099\n",
      "==============================================================================\n",
      "Omnibus:                      504.208   Durbin-Watson:                   1.964\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              931.363\n",
      "Skew:                           0.959   Prob(JB):                    5.72e-203\n",
      "Kurtosis:                       4.754   Cond. No.                         24.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "uncertainty – Treat2A:\n",
      "  Coefficient: 0.447\n",
      "  Std. Error: 0.109\n",
      "  t-stat: 4.10\n",
      "\n",
      "uncertainty – Prior_T2:\n",
      "  Coefficient: -0.495\n",
      "  Std. Error: 0.067\n",
      "  t-stat: -7.37\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ff7125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huber weights created. Summary:\n",
      "count      1368.000000\n",
      "mean      18888.982773\n",
      "std       14738.318447\n",
      "min        1232.619914\n",
      "25%       10021.700000\n",
      "50%       15613.851000\n",
      "75%       22981.133000\n",
      "max      108552.610000\n",
      "Name: Huber_Weight, dtype: float64\n",
      "CesDataFinal.csv updated with 'Huber_Weight' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1) Load your existing dataset\n",
    "# -----------------------------------------\n",
    "df = pd.read_csv(\"CesDataFinal.csv\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Ensure needed variables exist & are numeric\n",
    "# -----------------------------------------\n",
    "needed = [\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"First_Moment_Expectation_Post\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Second_Moment_Post\",\n",
    "    \"Second_Moment_Treatment\",\n",
    "    \"Control_Group\",\n",
    "    \"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\",\n",
    "    \"Weight_Oct\"\n",
    "]\n",
    "\n",
    "for col in needed:\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Required column '{col}' is missing from CesDataFinal.\")\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing key stuff\n",
    "df = df.dropna(subset=[\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"First_Moment_Expectation_Post\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Second_Moment_Post\",\n",
    "    \"Weight_Oct\"\n",
    "])\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3) Define treatment dummy (only T3 vs control here)\n",
    "# -----------------------------------------\n",
    "# Make sure they are 0/1\n",
    "df[\"T3\"] = df[\"Second_Moment_Treatment\"].fillna(0).astype(int)\n",
    "df[\"C\"]  = df[\"Control_Group\"].fillna(0).astype(int)\n",
    "\n",
    "# Sanity: every obs should be either control or T3 (or both 0 if you kept others)\n",
    "# print((df[\"T3\"] + df[\"C\"]).value_counts())\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4) Build prior × treatment interactions\n",
    "# -----------------------------------------\n",
    "df[\"Prior_Mean\"] = df[\"First_Moment_Expectation_Prior\"]\n",
    "df[\"Prior_Unc\"]  = df[\"Second_Moment_Prior\"]\n",
    "\n",
    "df[\"Prior_Mean_T3\"] = df[\"Prior_Mean\"] * df[\"T3\"]\n",
    "df[\"Prior_Unc_T3\"]  = df[\"Prior_Unc\"]  * df[\"T3\"]\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5) Huber-style robust regression to get weights\n",
    "#    (approximate Stata rreg2 logic)\n",
    "# -----------------------------------------\n",
    "w_base = df[\"Weight_Oct\"]\n",
    "sqrt_w = np.sqrt(w_base)\n",
    "\n",
    "# ---------- (a) Posterior mean equation ----------\n",
    "Y1 = df[\"First_Moment_Expectation_Post\"] * sqrt_w\n",
    "\n",
    "X1 = pd.DataFrame({\n",
    "    \"const\": sqrt_w,\n",
    "    \"Prior_Mean\": df[\"Prior_Mean\"] * sqrt_w,\n",
    "    \"Prior_Unc\":  df[\"Prior_Unc\"]  * sqrt_w,\n",
    "    \"T3\":         df[\"T3\"]         * sqrt_w,\n",
    "    \"Prior_Mean_T3\": df[\"Prior_Mean_T3\"] * sqrt_w,\n",
    "    \"Prior_Unc_T3\":  df[\"Prior_Unc_T3\"]  * sqrt_w,\n",
    "})\n",
    "\n",
    "# add country dummies (all) multiplied by sqrt_w\n",
    "for c in [\"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"]:\n",
    "    X1[f\"_{c}\"] = df[c] * sqrt_w\n",
    "\n",
    "rlm1 = sm.RLM(Y1, X1, M=sm.robust.norms.HuberT())\n",
    "res1 = rlm1.fit()\n",
    "w_huber1 = res1.weights  # ~ _wgt1 in Stata\n",
    "\n",
    "# ---------- (b) Posterior uncertainty equation ----------\n",
    "Y2 = df[\"Second_Moment_Post\"] * sqrt_w\n",
    "\n",
    "X2 = pd.DataFrame({\n",
    "    \"const\": sqrt_w,\n",
    "    \"Prior_Mean\": df[\"Prior_Mean\"] * sqrt_w,\n",
    "    \"Prior_Unc\":  df[\"Prior_Unc\"]  * sqrt_w,\n",
    "    \"T3\":         df[\"T3\"]         * sqrt_w,\n",
    "    \"Prior_Mean_T3\": df[\"Prior_Mean_T3\"] * sqrt_w,\n",
    "    \"Prior_Unc_T3\":  df[\"Prior_Unc_T3\"]  * sqrt_w,\n",
    "})\n",
    "for c in [\"Belgian\", \"Danish\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"]:\n",
    "    X2[f\"_{c}\"] = df[c] * sqrt_w\n",
    "\n",
    "rlm2 = sm.RLM(Y2, X2, M=sm.robust.norms.HuberT())\n",
    "res2 = rlm2.fit()\n",
    "w_huber2 = res2.weights  # ~ _wgt2 in Stata\n",
    "\n",
    "# -----------------------------------------\n",
    "# 6) Combine into final Huber-adjusted weight\n",
    "#    Stata: _wgt = sqrt(_wgt1) * sqrt(_wgt2) * wgt\n",
    "# -----------------------------------------\n",
    "df[\"Huber_Weight\"] = np.sqrt(w_huber1) * np.sqrt(w_huber2) * w_base\n",
    "\n",
    "print(\"Huber weights created. Summary:\")\n",
    "print(df[\"Huber_Weight\"].describe())\n",
    "\n",
    "# -----------------------------------------\n",
    "# 7) Save back to the SAME dataset file\n",
    "# -----------------------------------------\n",
    "df.to_csv(\"CesDataFinal.csv\", index=False)\n",
    "print(\"CesDataFinal.csv updated with 'Huber_Weight' column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00efd8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: linearmodels in c:\\users\\zach\\anaconda3\\lib\\site-packages (6.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from linearmodels) (2.3.4)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from linearmodels) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from linearmodels) (1.16.3)\n",
      "Requirement already satisfied: statsmodels>=0.13.0 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from linearmodels) (0.14.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.4 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from linearmodels) (1.0.0)\n",
      "Requirement already satisfied: Cython>=3.0.10 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from linearmodels) (3.1.3)\n",
      "Requirement already satisfied: pyhdfe>=0.1 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from linearmodels) (0.2.0)\n",
      "Requirement already satisfied: formulaic>=1.0.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from linearmodels) (1.2.0)\n",
      "Requirement already satisfied: setuptools-scm[toml]<9.0.0,>=8.0.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from linearmodels) (8.3.1)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from formulaic>=1.0.0->linearmodels) (1.3.0)\n",
      "Requirement already satisfied: narwhals>=1.17 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from formulaic>=1.0.0->linearmodels) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from formulaic>=1.0.0->linearmodels) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from formulaic>=1.0.0->linearmodels) (1.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (25.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zach\\anaconda3\\lib\\site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (68.0.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from statsmodels>=0.13.0->linearmodels) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f80e98a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-sample IV (Huber-style weights, before jackknife trimming):\n",
      "  fitted_First_Moment_Expectation_Post: coef = -0.7607, SE = 0.7984\n",
      "  fitted_Second_Moment_Post: coef = -5.3456, SE = 2.5362\n",
      "\n",
      "Observations before trimming: 3273\n",
      "Observations after trimming:  3262\n",
      "\n",
      "Final IV (Huber-style weights + jackknife trimming):\n",
      "  First_Moment_Post  (coef on fitted_First_Moment_Expectation_Post): -0.7337, SE = 0.8114\n",
      "  Second_Moment_Post (coef on fitted_Second_Moment_Post): -4.7911, SE = 2.2675\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "\n",
    "# ===================================================\n",
    "# 1) Load data\n",
    "# ===================================================\n",
    "df = pd.read_csv(\"CesDataTable3.csv\")\n",
    "\n",
    "# ===================================================\n",
    "# 2) Basic derived variables & treatment arms\n",
    "# ===================================================\n",
    "\n",
    "# Age^2/100 safeguard\n",
    "if \"Age_Squared\" not in df.columns and \"Age\" in df.columns:\n",
    "    df[\"Age_Squared\"] = (df[\"Age\"] ** 2) / 100.0\n",
    "\n",
    "# Treatment dummies: treat1 = control, treat2-5 = treatment arms\n",
    "# You renamed:\n",
    "#  - Control_Group              (treat1)\n",
    "#  - First_Moment_Treatment     (treat2)\n",
    "#  - Second_Moment_Treatment    (treat3)\n",
    "#  - Combined_Moments_Treatment (treat4)\n",
    "#  - Alternative_Treatment      (treat5)\n",
    "\n",
    "# treat2A\n",
    "df[\"treat2A\"] = np.nan\n",
    "df.loc[df[\"First_Moment_Treatment\"] == 1, \"treat2A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"Control_Group\"] == 1)\n",
    "    | (df[\"Second_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Combined_Moments_Treatment\"] == 1)\n",
    "    | (df[\"Alternative_Treatment\"] == 1),\n",
    "    \"treat2A\"\n",
    "] = 0\n",
    "\n",
    "# treat3A\n",
    "df[\"treat3A\"] = np.nan\n",
    "df.loc[df[\"Second_Moment_Treatment\"] == 1, \"treat3A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"Control_Group\"] == 1)\n",
    "    | (df[\"First_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Combined_Moments_Treatment\"] == 1)\n",
    "    | (df[\"Alternative_Treatment\"] == 1),\n",
    "    \"treat3A\"\n",
    "] = 0\n",
    "\n",
    "# treat4A\n",
    "df[\"treat4A\"] = np.nan\n",
    "df.loc[df[\"Combined_Moments_Treatment\"] == 1, \"treat4A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"Control_Group\"] == 1)\n",
    "    | (df[\"First_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Second_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Alternative_Treatment\"] == 1),\n",
    "    \"treat4A\"\n",
    "] = 0\n",
    "\n",
    "# treat5A (only for Huber first stage; not in final IV instrument set)\n",
    "df[\"treat5A\"] = np.nan\n",
    "df.loc[df[\"Alternative_Treatment\"] == 1, \"treat5A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"Control_Group\"] == 1)\n",
    "    | (df[\"First_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Second_Moment_Treatment\"] == 1)\n",
    "    | (df[\"Combined_Moments_Treatment\"] == 1),\n",
    "    \"treat5A\"\n",
    "] = 0\n",
    "\n",
    "# Expectations: prior (bt) and post (pt)\n",
    "xvar1_bt = \"First_Moment_Expectation_Prior\"\n",
    "xvar1_pt = \"First_Moment_Expectation_Post\"\n",
    "xvar2_bt = \"Second_Moment_Prior\"\n",
    "xvar2_pt = \"Second_Moment_Post\"\n",
    "\n",
    "# initi_ variables (within-ID means of priors; with 1 obs/ID = value itself)\n",
    "df[\"initi_egrea_imean\"] = df[xvar1_bt]\n",
    "df[\"initi_egrea_istd\"] = df[xvar2_bt]\n",
    "\n",
    "# Interaction instruments for IV (treat2-4 only)\n",
    "df[\"initXi2_egrea_imean\"] = df[xvar1_bt] * df[\"treat2A\"]\n",
    "df[\"initXi3_egrea_imean\"] = df[xvar1_bt] * df[\"treat3A\"]\n",
    "df[\"initXi4_egrea_imean\"] = df[xvar1_bt] * df[\"treat4A\"]\n",
    "\n",
    "df[\"initXi2_egrea_istd\"] = df[xvar2_bt] * df[\"treat2A\"]\n",
    "df[\"initXi3_egrea_istd\"] = df[xvar2_bt] * df[\"treat3A\"]\n",
    "df[\"initXi4_egrea_istd\"] = df[xvar2_bt] * df[\"treat4A\"]\n",
    "\n",
    "# For Huber weight generation, also include treat5 / initXi5 for the chosen var\n",
    "df[\"initXi5_egrea_imean\"] = df[xvar1_bt] * df[\"treat5A\"]\n",
    "\n",
    "# ===================================================\n",
    "# 3) DV and regressors for IV (you said these are OK)\n",
    "# ===================================================\n",
    "\n",
    "# Dependent variable: your \"good\" DV (log net consumption * 100)\n",
    "yvar = \"Log_Consumption_Jan\"\n",
    "df[\"y\"] = df[yvar] * 100.0\n",
    "\n",
    "# spec0 controls\n",
    "spec0 = [\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    \"Age\",\n",
    "    \"Age_Squared\",\n",
    "    \"Household_Size\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Liquidity_Oct\",\n",
    "    \"Male\",\n",
    "    \"Growth_Uncertainty_Probability\",\n",
    "    \"Planned_Spending_Consumer_Goods\",\n",
    "    \"Planned_Spending_Durable_Goods\",\n",
    "]\n",
    "\n",
    "# Country dummies in Python: Belgian, Danish, Spanish, French, Italian, Dutch\n",
    "# Final IV spec (like cnt1, cnt3, cnt4, cnt5, cnt6) drops Danish as base\n",
    "cnt_iv = [\"Belgian\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"]\n",
    "\n",
    "# Exogenous regressors in IV\n",
    "exog_vars = spec0 + [\"initi_egrea_imean\", \"initi_egrea_istd\"] + cnt_iv\n",
    "\n",
    "# Endogenous regressors: post expectations\n",
    "endog_vars = [xvar1_pt, xvar2_pt]\n",
    "\n",
    "# Extra IV instruments (no treat5A here; matches second-stage Stata spec)\n",
    "instr_extra = [\n",
    "    \"initXi2_egrea_imean\",\n",
    "    \"initXi3_egrea_imean\",\n",
    "    \"initXi4_egrea_imean\",\n",
    "    \"initXi2_egrea_istd\",\n",
    "    \"initXi3_egrea_istd\",\n",
    "    \"initXi4_egrea_istd\",\n",
    "    \"treat2A\",\n",
    "    \"treat3A\",\n",
    "    \"treat4A\",\n",
    "]\n",
    "\n",
    "id_var = \"HH_ID\"\n",
    "\n",
    "# Base survey weight corresponds to Stata wgt (wave 9)\n",
    "base_weight_var = \"Weight_Expectations\"\n",
    "\n",
    "# ===================================================\n",
    "# 4) Restrict to complete-case sample (like e(sample))\n",
    "# ===================================================\n",
    "needed_cols = (\n",
    "    [\"y\", id_var, base_weight_var]\n",
    "    + exog_vars\n",
    "    + endog_vars\n",
    "    + instr_extra\n",
    "    + [\"treat5A\", \"initXi5_egrea_imean\"]  # used only for Huber step\n",
    ")\n",
    "df_reg = df[needed_cols].dropna().copy()\n",
    "df_reg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ===================================================\n",
    "# 5) Recreate Huber-type weights à la Stata snippet\n",
    "#     First stage: robust regression of First_Moment_Post\n",
    "# ===================================================\n",
    "\n",
    "# In Stata:\n",
    "#   _Y1 = var_pt * sqrt(wgt)\n",
    "#   _W  = sqrt(wgt)\n",
    "#   _X1 = initi_var * sqrt(wgt)\n",
    "#   _M1.._M4 = treat2A..treat5A * sqrt(wgt)\n",
    "#   _Z1.._Z4 = initXi2..initXi5_var * sqrt(wgt)\n",
    "#   _cnt*    = cnt1..cnt6 * sqrt(wgt)\n",
    "#   rreg2 _Y1 _X1 _W _M1.._M4 _Z1.._Z4 _cnt* , gen(_wgt1)\n",
    "#   _wgt = _wgt1 * wgt\n",
    "\n",
    "w = df_reg[base_weight_var].values\n",
    "sqrtw = np.sqrt(w)\n",
    "\n",
    "# Choose var = First_Moment_Expectation_Post (like egrea_imean_pt)\n",
    "var_pt = xvar1_pt\n",
    "initi_var = \"initi_egrea_imean\"\n",
    "\n",
    "# Weighted versions (underscore names mimic Stata)\n",
    "Y1 = df_reg[var_pt].values * sqrtw  # _Y1\n",
    "W_col = sqrtw                       # _W\n",
    "X1 = df_reg[initi_var].values * sqrtw  # _X1\n",
    "\n",
    "M1 = df_reg[\"treat2A\"].values * sqrtw\n",
    "M2 = df_reg[\"treat3A\"].values * sqrtw\n",
    "M3 = df_reg[\"treat4A\"].values * sqrtw\n",
    "M4 = df_reg[\"treat5A\"].values * sqrtw\n",
    "\n",
    "Z1 = df_reg[\"initXi2_egrea_imean\"].values * sqrtw\n",
    "Z2 = df_reg[\"initXi3_egrea_imean\"].values * sqrtw\n",
    "Z3 = df_reg[\"initXi4_egrea_imean\"].values * sqrtw\n",
    "Z4 = df_reg[\"initXi5_egrea_imean\"].values * sqrtw\n",
    "\n",
    "# Country dummies for Huber first stage: use all 6 (Belgian..Dutch)\n",
    "cnt_all = [\"Belgian\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"]\n",
    "cnt_cols = [df_reg[c].values * sqrtw for c in cnt_all]\n",
    "\n",
    "# Build X matrix for robust regression (no extra constant; W_col acts like it)\n",
    "X_rlm_cols = [X1, W_col, M1, M2, M3, M4, Z1, Z2, Z3, Z4] + cnt_cols\n",
    "X_rlm = np.column_stack(X_rlm_cols)\n",
    "\n",
    "# Robust regression (Huber-type) to get robustness weights _wgt1\n",
    "rlm_mod = sm.RLM(Y1, X_rlm, M=HuberT())\n",
    "rlm_res = rlm_mod.fit()\n",
    "\n",
    "# Robustness weights (analogous to _wgt1 from rreg2)\n",
    "wgt1 = rlm_res.weights  # 1 for well-behaved obs, <1 for outliers\n",
    "\n",
    "# Final analytic weight _wgt = _wgt1 * wgt\n",
    "df_reg[\"_wgt\"] = wgt1 * w\n",
    "\n",
    "weight_var = \"_wgt\"\n",
    "\n",
    "# ===================================================\n",
    "# 6) Helper: IV 2SLS with analytic weights & clustered SE\n",
    "# ===================================================\n",
    "def run_iv_2sls(df_sub):\n",
    "    \"\"\"\n",
    "    df_sub must contain:\n",
    "      y, _wgt, HH_ID, exog_vars, endog_vars, instr_extra\n",
    "    Uses:\n",
    "      - First stage: WLS of each endogenous variable on instruments\n",
    "      - Second stage: WLS of y on exog + fitted endogenous\n",
    "      - Cluster-robust SEs by HH_ID\n",
    "    \"\"\"\n",
    "    y = df_sub[\"y\"]\n",
    "    w = df_sub[weight_var]\n",
    "    clusters = df_sub[id_var]\n",
    "\n",
    "    # Instruments Z = exog + extra instruments (no constant; we add later)\n",
    "    Z = pd.concat([df_sub[exog_vars], df_sub[instr_extra]], axis=1)\n",
    "    Z = sm.add_constant(Z, has_constant=\"add\")\n",
    "\n",
    "    # First stage: regress each endogenous var on Z with weights\n",
    "    fitted_endog = pd.DataFrame(index=df_sub.index)\n",
    "    for v in endog_vars:\n",
    "        fs_model = sm.WLS(df_sub[v], Z, weights=w)\n",
    "        fs_res = fs_model.fit()\n",
    "        fitted_endog[\"fitted_\" + v] = fs_res.fittedvalues\n",
    "\n",
    "    # Second stage: regress y on exog + fitted endogenous with weights\n",
    "    X_2sls = pd.concat([df_sub[exog_vars], fitted_endog], axis=1)\n",
    "    X_2sls = sm.add_constant(X_2sls, has_constant=\"add\")\n",
    "\n",
    "    ss_model = sm.WLS(y, X_2sls, weights=w)\n",
    "    ss_res = ss_model.fit(\n",
    "        cov_type=\"cluster\",\n",
    "        cov_kwds={\"groups\": clusters}\n",
    "    )\n",
    "    return ss_res\n",
    "\n",
    "# ===================================================\n",
    "# 7) Full-sample IV (for jackknife normalization)\n",
    "# ===================================================\n",
    "full_res = run_iv_2sls(df_reg)\n",
    "\n",
    "param1 = \"fitted_\" + xvar1_pt  # First_Moment_Post\n",
    "param2 = \"fitted_\" + xvar2_pt  # Second_Moment_Post\n",
    "\n",
    "b1_full = full_res.params[param1]\n",
    "se1_full = full_res.bse[param1]\n",
    "b2_full = full_res.params[param2]\n",
    "se2_full = full_res.bse[param2]\n",
    "\n",
    "print(\"Full-sample IV (Huber-style weights, before jackknife trimming):\")\n",
    "print(f\"  {param1}: coef = {b1_full:.4f}, SE = {se1_full:.4f}\")\n",
    "print(f\"  {param2}: coef = {b2_full:.4f}, SE = {se2_full:.4f}\")\n",
    "print()\n",
    "\n",
    "# ===================================================\n",
    "# 8) Leave-one-out jackknife: _c_lc1, _c_lc2\n",
    "# ===================================================\n",
    "n = df_reg.shape[0]\n",
    "c1 = np.empty(n)\n",
    "c2 = np.empty(n)\n",
    "\n",
    "for i in range(n):\n",
    "    df_j = df_reg.drop(index=i)\n",
    "    res_j = run_iv_2sls(df_j)\n",
    "    c1[i] = res_j.params[param1]\n",
    "    c2[i] = res_j.params[param2]\n",
    "\n",
    "df_reg[\"_c_lc1\"] = c1   # jackknife coef for First_Moment_Post\n",
    "df_reg[\"_c_lc2\"] = c2   # jackknife coef for Second_Moment_Post\n",
    "\n",
    "# ===================================================\n",
    "# 9) Normalize jackknife coefs and trim like Stata\n",
    "# ===================================================\n",
    "df_reg[\"_c_lc1_norm\"] = (df_reg[\"_c_lc1\"] - b1_full) / se1_full\n",
    "df_reg[\"_c_lc2_norm\"] = (df_reg[\"_c_lc2\"] - b2_full) / se2_full\n",
    "\n",
    "trim_mask = (df_reg[\"_c_lc1_norm\"].abs() + df_reg[\"_c_lc2_norm\"].abs()) < 0.25\n",
    "df_trim = df_reg[trim_mask].copy()\n",
    "\n",
    "print(f\"Observations before trimming: {df_reg.shape[0]}\")\n",
    "print(f\"Observations after trimming:  {df_trim.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# ===================================================\n",
    "# 10) Final IV on trimmed sample\n",
    "# ===================================================\n",
    "final_res = run_iv_2sls(df_trim)\n",
    "\n",
    "b1_final = final_res.params[param1]\n",
    "se1_final = final_res.bse[param1]\n",
    "b2_final = final_res.params[param2]\n",
    "se2_final = final_res.bse[param2]\n",
    "\n",
    "print(\"Final IV (Huber-style weights + jackknife trimming):\")\n",
    "print(f\"  First_Moment_Post  (coef on {param1}): {b1_final:.4f}, SE = {se1_final:.4f}\")\n",
    "print(f\"  Second_Moment_Post (coef on {param2}): {b2_final:.4f}, SE = {se2_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86594b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstatNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/c7/9e/ea38426a1d0171cfd1a48dcd00a7cbd4523d2e9964c387083887e94a36be/pyreadstat-1.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.3.2-cp311-cp311-win_amd64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: narwhals>=2.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from pyreadstat) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\zach\\appdata\\roaming\\python\\python311\\site-packages (from pyreadstat) (2.3.4)\n",
      "Downloading pyreadstat-1.3.2-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.4 MB 653.6 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.4 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f11130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "\n",
    "stata_path = \"ces_gdp_rct_v1a.dta\"  # adjust path if needed\n",
    "df, meta = pyreadstat.read_dta(stata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4855fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 9 observations after dropping treat5 and trimming lc: 8203\n",
      "Rows used in Huber first stage: 7553\n",
      "Rows with non-missing Huber weight: 7553\n",
      "Rows used in IV sample: 5906\n",
      "\n",
      "Full-sample IV (Huber-weighted, pre-jackknife trim):\n",
      "  First_Moment_Post  (coef on fitted_egrea_imean_pt): -0.9107, SE = 0.8491\n",
      "  Second_Moment_Post (coef on fitted_egrea_istd_pt): -6.2813, SE = 2.3641\n",
      "\n",
      "Running jackknife (leave-one-out over IV sample)...\n",
      "\n",
      "Jackknife normalization and trimming:\n",
      "  Observations before trimming: 5906\n",
      "  Observations after trimming:  5900\n",
      "\n",
      "Final IV (Huber-weighted + jackknife trimming):\n",
      "  First_Moment_Post  (coef on fitted_egrea_imean_pt): -0.8554, SE = 0.7370\n",
      "  Second_Moment_Post (coef on fitted_egrea_istd_pt): -4.1271, SE = 1.9956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "import pyreadstat\n",
    "\n",
    "# ===================================================\n",
    "# 0) Load Stata data and clean basic structure\n",
    "# ===================================================\n",
    "\n",
    "stata_path = \"ces_gdp_rct_v1a.dta\"   # adjust path as needed\n",
    "df, meta = pyreadstat.read_dta(stata_path)\n",
    "\n",
    "# Drop duplicate column names if any (keep first occurrence)\n",
    "df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Force key variables to numeric\n",
    "# ---------------------------------------------------\n",
    "numeric_cols = [\n",
    "    \"hid\", \"wave\", \"wgt\",\n",
    "    \"cons_tot\", \"cons_dbt\",\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\", \"treat5\",\n",
    "    \"dedu2\", \"dedu3\",\n",
    "    \"age\", \"hhsize\", \"lhhnetinc\",\n",
    "    \"liquid\", \"male\", \"k1010_3\",\n",
    "    \"pr2010\", \"pr2110\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# ===================================================\n",
    "# 1) Drop treatment 4 households (paper: drop \"treatment 4\")\n",
    "#    In this dataset that corresponds to treat5 == 1\n",
    "# ===================================================\n",
    "\n",
    "df = df[df[\"treat5\"] != 1].copy()\n",
    "\n",
    "# ===================================================\n",
    "# 2) Construct DV: lc = log(f1.cons_tot - f1.cons_dbt) at wave 9\n",
    "# ===================================================\n",
    "\n",
    "# Sort by ID and wave like tsset hid wave\n",
    "df = df.sort_values([\"hid\", \"wave\"])\n",
    "\n",
    "# Lead values within hid: f1.cons_tot and f1.cons_dbt\n",
    "df[\"cons_tot_lead\"] = df.groupby(\"hid\")[\"cons_tot\"].shift(-1)\n",
    "df[\"cons_dbt_lead\"] = df.groupby(\"hid\")[\"cons_dbt\"].shift(-1)\n",
    "\n",
    "# Net future consumption\n",
    "df[\"net_cons_lead\"] = df[\"cons_tot_lead\"] - df[\"cons_dbt_lead\"]\n",
    "df[\"net_cons_lead\"] = pd.to_numeric(df[\"net_cons_lead\"], errors=\"coerce\")\n",
    "\n",
    "# Only positive values make sense inside log\n",
    "df.loc[df[\"net_cons_lead\"] <= 0, \"net_cons_lead\"] = np.nan\n",
    "\n",
    "# Log\n",
    "df[\"lc\"] = np.log(df[\"net_cons_lead\"])\n",
    "\n",
    "# Trim extremes: lc < 5 or lc > 10 → missing, as in Stata\n",
    "df.loc[df[\"lc\"] < 5, \"lc\"] = np.nan\n",
    "df.loc[df[\"lc\"] > 10, \"lc\"] = np.nan\n",
    "\n",
    "# Keep only wave == 9\n",
    "df = df[df[\"wave\"] == 9].copy()\n",
    "\n",
    "# Final DV (they multiply by 100 later)\n",
    "df[\"y\"] = df[\"lc\"] * 100.0\n",
    "\n",
    "print(\"Wave 9 observations after dropping treat5 and trimming lc:\", df.shape[0])\n",
    "\n",
    "# ===================================================\n",
    "# 3) Controls, country dummies, and transformations\n",
    "# ===================================================\n",
    "\n",
    "# Age squared / 100\n",
    "df[\"age2\"] = (df[\"age\"] ** 2) / 100.0\n",
    "\n",
    "# Planned spending dummies\n",
    "df[\"pr2010D\"] = np.where(df[\"pr2010\"].notna(), (df[\"pr2010\"] == 2).astype(float), np.nan)\n",
    "df[\"pr2110D\"] = np.where(df[\"pr2110\"].notna(), (df[\"pr2110\"] == 2).astype(float), np.nan)\n",
    "\n",
    "# spec0 controls (as in Stata global spec0)\n",
    "spec0 = [\n",
    "    \"dedu2\",\n",
    "    \"dedu3\",\n",
    "    \"age\",\n",
    "    \"age2\",\n",
    "    \"hhsize\",\n",
    "    \"lhhnetinc\",\n",
    "    \"liquid\",\n",
    "    \"male\",\n",
    "    \"k1010_3\",\n",
    "    \"pr2010D\",\n",
    "    \"pr2110D\",\n",
    "]\n",
    "\n",
    "# Country dummies\n",
    "cnt_iv = [c for c in [\"cnt1\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"] if c in df.columns]\n",
    "cnt_all = [c for c in [\"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"] if c in df.columns]\n",
    "\n",
    "# ===================================================\n",
    "# 4) Treatment dummies and expectation structure\n",
    "# ===================================================\n",
    "\n",
    "# treat2A\n",
    "df[\"treat2A\"] = np.nan\n",
    "df.loc[df[\"treat2\"] == 1, \"treat2A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"treat1\"] == 1)\n",
    "    | (df[\"treat3\"] == 1)\n",
    "    | (df[\"treat4\"] == 1)\n",
    "    | (df[\"treat5\"] == 1),\n",
    "    \"treat2A\"\n",
    "] = 0\n",
    "\n",
    "# treat3A\n",
    "df[\"treat3A\"] = np.nan\n",
    "df.loc[df[\"treat3\"] == 1, \"treat3A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"treat1\"] == 1)\n",
    "    | (df[\"treat2\"] == 1)\n",
    "    | (df[\"treat4\"] == 1)\n",
    "    | (df[\"treat5\"] == 1),\n",
    "    \"treat3A\"\n",
    "] = 0\n",
    "\n",
    "# treat4A\n",
    "df[\"treat4A\"] = np.nan\n",
    "df.loc[df[\"treat4\"] == 1, \"treat4A\"] = 1\n",
    "df.loc[\n",
    "    (df[\"treat1\"] == 1)\n",
    "    | (df[\"treat2\"] == 1)\n",
    "    | (df[\"treat3\"] == 1)\n",
    "    | (df[\"treat5\"] == 1),\n",
    "    \"treat4A\"\n",
    "] = 0\n",
    "\n",
    "# Expectations variables\n",
    "xvar1_bt = \"egrea_imean_bt\"\n",
    "xvar1_pt = \"egrea_imean_pt\"\n",
    "xvar2_bt = \"egrea_istd_bt\"\n",
    "xvar2_pt = \"egrea_istd_pt\"\n",
    "\n",
    "# initi_ variables: ID-level means of *_bt at wave 9\n",
    "df[\"initi_egrea_imean\"] = df.groupby(\"hid\")[xvar1_bt].transform(\"mean\")\n",
    "df[\"initi_egrea_istd\"]  = df.groupby(\"hid\")[xvar2_bt].transform(\"mean\")\n",
    "\n",
    "# Interaction instruments for xvar1 (mean)\n",
    "tmp = df[xvar1_bt] * df[\"treat2A\"]\n",
    "df[\"initXi2_egrea_imean\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "tmp = df[xvar1_bt] * df[\"treat3A\"]\n",
    "df[\"initXi3_egrea_imean\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "tmp = df[xvar1_bt] * df[\"treat4A\"]\n",
    "df[\"initXi4_egrea_imean\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "# For Huber step we also need treat5A interaction; treat5A is zero now but included for structure\n",
    "df[\"treat5A\"] = 0.0\n",
    "tmp = df[xvar1_bt] * df[\"treat5A\"]\n",
    "df[\"initXi5_egrea_imean\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "# Interaction instruments for xvar2 (uncertainty)\n",
    "tmp = df[xvar2_bt] * df[\"treat2A\"]\n",
    "df[\"initXi2_egrea_istd\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "tmp = df[xvar2_bt] * df[\"treat3A\"]\n",
    "df[\"initXi3_egrea_istd\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "tmp = df[xvar2_bt] * df[\"treat4A\"]\n",
    "df[\"initXi4_egrea_istd\"] = tmp.groupby(df[\"hid\"]).transform(\"mean\")\n",
    "\n",
    "base_weight_var = \"wgt\"\n",
    "id_var = \"hid\"\n",
    "\n",
    "# ===================================================\n",
    "# 5) Huber-style weights: approximate Stata rreg2 (_wgt1 * wgt)\n",
    "# ===================================================\n",
    "\n",
    "huber_cols = [\n",
    "    base_weight_var,\n",
    "    xvar1_pt,\n",
    "    \"initi_egrea_imean\",\n",
    "    \"treat2A\", \"treat3A\", \"treat4A\", \"treat5A\",\n",
    "    \"initXi2_egrea_imean\",\n",
    "    \"initXi3_egrea_imean\",\n",
    "    \"initXi4_egrea_imean\",\n",
    "    \"initXi5_egrea_imean\",\n",
    "] + cnt_all\n",
    "\n",
    "# 1) Coerce huber_cols to numeric\n",
    "for c in huber_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# 2) Drop rows with any missing in huber_cols and wgt <= 0\n",
    "df_huber = df[huber_cols].copy()\n",
    "mask_finite = np.isfinite(df_huber.to_numpy()).all(axis=1)\n",
    "df_huber = df.loc[mask_finite].copy()\n",
    "df_huber = df_huber[df_huber[base_weight_var] > 0].copy()\n",
    "\n",
    "print(\"Rows used in Huber first stage:\", df_huber.shape[0])\n",
    "\n",
    "# 3) Build weighted variables (Y1 and X matrix)\n",
    "w = df_huber[base_weight_var].to_numpy(dtype=float)\n",
    "sqrtw = np.sqrt(w)\n",
    "\n",
    "Y1 = df_huber[xvar1_pt].to_numpy(dtype=float) * sqrtw\n",
    "W_col = sqrtw\n",
    "X1 = df_huber[\"initi_egrea_imean\"].to_numpy(dtype=float) * sqrtw\n",
    "\n",
    "M1 = df_huber[\"treat2A\"].to_numpy(dtype=float) * sqrtw\n",
    "M2 = df_huber[\"treat3A\"].to_numpy(dtype=float) * sqrtw\n",
    "M3 = df_huber[\"treat4A\"].to_numpy(dtype=float) * sqrtw\n",
    "M4 = df_huber[\"treat5A\"].to_numpy(dtype=float) * sqrtw\n",
    "\n",
    "Z1 = df_huber[\"initXi2_egrea_imean\"].to_numpy(dtype=float) * sqrtw\n",
    "Z2 = df_huber[\"initXi3_egrea_imean\"].to_numpy(dtype=float) * sqrtw\n",
    "Z3 = df_huber[\"initXi4_egrea_imean\"].to_numpy(dtype=float) * sqrtw\n",
    "Z4 = df_huber[\"initXi5_egrea_imean\"].to_numpy(dtype=float) * sqrtw\n",
    "\n",
    "cnt_cols = []\n",
    "for c in cnt_all:\n",
    "    if c in df_huber.columns:\n",
    "        cnt_cols.append(df_huber[c].to_numpy(dtype=float) * sqrtw)\n",
    "\n",
    "X_rlm_cols = [X1, W_col, M1, M2, M3, M4, Z1, Z2, Z3, Z4] + cnt_cols\n",
    "X_rlm = np.column_stack(X_rlm_cols)\n",
    "\n",
    "# 4) Robust regression to get Huber weights\n",
    "rlm_mod = sm.RLM(Y1, X_rlm, M=HuberT())\n",
    "rlm_res = rlm_mod.fit()\n",
    "\n",
    "wgt1 = rlm_res.weights   # like Stata's _wgt1\n",
    "\n",
    "# 5) Attach _wgt = _wgt1 * wgt back to df\n",
    "df[\"_wgt\"] = np.nan\n",
    "df.loc[df_huber.index, \"_wgt\"] = wgt1 * w\n",
    "\n",
    "# Keep only rows with non-missing _wgt for the IV stage\n",
    "df = df[df[\"_wgt\"].notna()].copy()\n",
    "\n",
    "print(\"Rows with non-missing Huber weight:\", df.shape[0])\n",
    "\n",
    "# ===================================================\n",
    "# 6) Build IV design: exogenous, endogenous, instruments\n",
    "# ===================================================\n",
    "\n",
    "exog_vars = spec0 + [\"initi_egrea_imean\", \"initi_egrea_istd\"] + cnt_iv\n",
    "endog_vars = [xvar1_pt, xvar2_pt]\n",
    "instr_extra = [\n",
    "    \"initXi2_egrea_imean\",\n",
    "    \"initXi3_egrea_imean\",\n",
    "    \"initXi4_egrea_imean\",\n",
    "    \"initXi2_egrea_istd\",\n",
    "    \"initXi3_egrea_istd\",\n",
    "    \"initXi4_egrea_istd\",\n",
    "    \"treat2A\",\n",
    "    \"treat3A\",\n",
    "    \"treat4A\",\n",
    "]\n",
    "\n",
    "needed_cols = [\"y\", id_var, \"_wgt\"] + exog_vars + endog_vars + instr_extra\n",
    "needed_cols = [c for c in needed_cols if c in df.columns]\n",
    "\n",
    "df_reg = df[needed_cols].copy()\n",
    "\n",
    "# Make everything numeric for the IV\n",
    "for c in needed_cols:\n",
    "    df_reg[c] = pd.to_numeric(df_reg[c], errors=\"coerce\")\n",
    "\n",
    "df_reg = df_reg.dropna().copy()\n",
    "print(\"Rows used in IV sample:\", df_reg.shape[0])\n",
    "\n",
    "weight_var = \"_wgt\"\n",
    "\n",
    "# ===================================================\n",
    "# 7) Helper: run Huber-weighted 2SLS with clustered SEs\n",
    "# ===================================================\n",
    "\n",
    "def run_iv_2sls(df_sub):\n",
    "    y = df_sub[\"y\"].astype(float)\n",
    "    w = df_sub[weight_var].astype(float)\n",
    "    clusters = df_sub[id_var]\n",
    "\n",
    "    # Instruments: exog + extra instruments\n",
    "    Z = pd.concat([df_sub[exog_vars], df_sub[instr_extra]], axis=1)\n",
    "    Z = Z.astype(float)\n",
    "    Z = sm.add_constant(Z, has_constant=\"add\")\n",
    "\n",
    "    # First stage: each endogenous regressor on Z\n",
    "    fitted_endog = pd.DataFrame(index=df_sub.index)\n",
    "    for v in endog_vars:\n",
    "        fs_model = sm.WLS(df_sub[v].astype(float), Z, weights=w)\n",
    "        fs_res = fs_model.fit()\n",
    "        fitted_endog[\"fitted_\" + v] = fs_res.fittedvalues\n",
    "\n",
    "    # Second stage: y on exog + fitted endog\n",
    "    X_2sls = pd.concat([df_sub[exog_vars].astype(float), fitted_endog], axis=1)\n",
    "    X_2sls = sm.add_constant(X_2sls, has_constant=\"add\")\n",
    "\n",
    "    ss_model = sm.WLS(y, X_2sls, weights=w)\n",
    "    ss_res = ss_model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "    return ss_res\n",
    "\n",
    "# ===================================================\n",
    "# 8) Full-sample IV (for jackknife normalization)\n",
    "# ===================================================\n",
    "\n",
    "full_res = run_iv_2sls(df_reg)\n",
    "\n",
    "param1 = \"fitted_\" + xvar1_pt   # First_Moment_Post\n",
    "param2 = \"fitted_\" + xvar2_pt   # Second_Moment_Post\n",
    "\n",
    "b1_full = full_res.params[param1]\n",
    "se1_full = full_res.bse[param1]\n",
    "b2_full = full_res.params[param2]\n",
    "se2_full = full_res.bse[param2]\n",
    "\n",
    "print(\"\\nFull-sample IV (Huber-weighted, pre-jackknife trim):\")\n",
    "print(f\"  First_Moment_Post  (coef on {param1}): {b1_full:.4f}, SE = {se1_full:.4f}\")\n",
    "print(f\"  Second_Moment_Post (coef on {param2}): {b2_full:.4f}, SE = {se2_full:.4f}\")\n",
    "\n",
    "# ===================================================\n",
    "# 9) Leave-one-out jackknife (Coibion-style)\n",
    "# ===================================================\n",
    "\n",
    "n = df_reg.shape[0]\n",
    "c1 = np.full(n, np.nan)\n",
    "c2 = np.full(n, np.nan)\n",
    "\n",
    "print(\"\\nRunning jackknife (leave-one-out over IV sample)...\")\n",
    "\n",
    "for j, idx in enumerate(df_reg.index):\n",
    "    df_j = df_reg.drop(index=idx)\n",
    "    try:\n",
    "        res_j = run_iv_2sls(df_j)\n",
    "        c1[j] = res_j.params[param1]\n",
    "        c2[j] = res_j.params[param2]\n",
    "    except Exception as e:\n",
    "        # if a particular leave-one-out fails, leave NaN and continue\n",
    "        print(f\"  Jackknife step {j+1}/{n} failed: {e}\")\n",
    "\n",
    "df_reg[\"_c_lc1\"] = c1\n",
    "df_reg[\"_c_lc2\"] = c2\n",
    "\n",
    "# ===================================================\n",
    "# 10) Normalize jackknife coefs and trim\n",
    "# ===================================================\n",
    "\n",
    "df_reg[\"_c_lc1_norm\"] = (df_reg[\"_c_lc1\"] - b1_full) / se1_full\n",
    "df_reg[\"_c_lc2_norm\"] = (df_reg[\"_c_lc2\"] - b2_full) / se2_full\n",
    "\n",
    "trim_mask = (df_reg[\"_c_lc1_norm\"].abs() + df_reg[\"_c_lc2_norm\"].abs()) < 0.25\n",
    "df_trim = df_reg[trim_mask].copy()\n",
    "\n",
    "print(\"\\nJackknife normalization and trimming:\")\n",
    "print(\"  Observations before trimming:\", df_reg.shape[0])\n",
    "print(\"  Observations after trimming: \", df_trim.shape[0])\n",
    "\n",
    "# ===================================================\n",
    "# 11) Final IV on trimmed sample\n",
    "# ===================================================\n",
    "\n",
    "final_res = run_iv_2sls(df_trim)\n",
    "\n",
    "b1_final = final_res.params[param1]\n",
    "se1_final = final_res.bse[param1]\n",
    "b2_final = final_res.params[param2]\n",
    "se2_final = final_res.bse[param2]\n",
    "\n",
    "print(\"\\nFinal IV (Huber-weighted + jackknife trimming):\")\n",
    "print(f\"  First_Moment_Post  (coef on {param1}): {b1_final:.4f}, SE = {se1_final:.4f}\")\n",
    "print(f\"  Second_Moment_Post (coef on {param2}): {b2_final:.4f}, SE = {se2_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532548d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eb7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zach\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-sample IV (no trimming, DV = lc):\n",
      " First_Moment_Post (egrea_imean_pt):  beta = -0.001025949086920331   se = 0.009077687839136197\n",
      " Second_Moment_Post (egrea_istd_pt): beta = -0.055588830211632856   se = 0.020384840369361047\n",
      "\n",
      "Final IV (with Huber weights + jackknife trimming, DV = lc*100):\n",
      " First_Moment_Post (egrea_imean_pt):\n",
      "    beta = -0.7627566189231704\n",
      "    se   = 0.8452346358849411\n",
      " Second_Moment_Post (egrea_istd_pt):\n",
      "    beta = -4.473006775289798\n",
      "    se   = 1.9179309070819088\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) Imports\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "\n",
    "# (optional) small numpy 2.x shim for old code that uses np.unicode_\n",
    "if not hasattr(np, \"unicode_\"):\n",
    "    np.unicode_ = np.str_\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Helper functions\n",
    "# ============================================================\n",
    "\n",
    "def to_numeric(df, cols):\n",
    "    \"\"\"Coerce listed columns to numeric (in-place).\"\"\"\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "\n",
    "def wls_cluster(y, X, w, clusters):\n",
    "    \"\"\"\n",
    "    Weighted least squares with cluster-robust covariance.\n",
    "\n",
    "    y: (N,) vector\n",
    "    X: (N,K) matrix of regressors\n",
    "    w: (N,) weights (Stata [aw] style -> use sqrt(w) for WLS)\n",
    "    clusters: (N,) cluster IDs (e.g. ID)\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    X = np.asarray(X, float)\n",
    "    w = np.asarray(w, float)\n",
    "    clusters = np.asarray(clusters)\n",
    "\n",
    "    sw = np.sqrt(w)\n",
    "    Xw = X * sw[:, None]\n",
    "    yw = y * sw\n",
    "\n",
    "    XtX = Xw.T @ Xw\n",
    "    beta = np.linalg.solve(XtX, Xw.T @ yw)\n",
    "    e_w = yw - Xw @ beta\n",
    "\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    k = X.shape[1]\n",
    "    S = np.zeros((k, k))\n",
    "\n",
    "    for g in unique_clusters:\n",
    "        idx = np.where(clusters == g)[0]\n",
    "        Xg = Xw[idx, :]\n",
    "        eg = e_w[idx]\n",
    "        Sg = Xg.T @ eg\n",
    "        S += np.outer(Sg, Sg)\n",
    "\n",
    "    G = len(unique_clusters)\n",
    "    N = X.shape[0]\n",
    "    # standard finite-sample adjustment (like Stata's cluster-robust)\n",
    "    dof_adj = (G / (G - 1)) * ((N - 1) / (N - k))\n",
    "\n",
    "    inv_XtX = np.linalg.inv(XtX)\n",
    "    vcov = dof_adj * inv_XtX @ S @ inv_XtX\n",
    "\n",
    "    return beta, vcov\n",
    "\n",
    "\n",
    "def iv_2sls_cluster(y, exog, endog, instr, w, clusters,\n",
    "                    exog_names, endog_names):\n",
    "    \"\"\"\n",
    "    Two-stage least squares with weights and cluster-robust SEs.\n",
    "    Instruments = [exog, instr] (as in Stata: exog are their own instruments).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, float)\n",
    "    exog = np.asarray(exog, float)\n",
    "    endog = np.asarray(endog, float)\n",
    "    instr = np.asarray(instr, float)\n",
    "    w = np.asarray(w, float)\n",
    "    clusters = np.asarray(clusters)\n",
    "\n",
    "    # instrument matrix: exog + extra instruments\n",
    "    Z = np.column_stack([exog, instr])\n",
    "\n",
    "    # -------------- Stage 1: project each endogenous var on Z --------------\n",
    "    def wls_simple(y1, X1, w1):\n",
    "        sw1 = np.sqrt(w1)\n",
    "        Xw1 = X1 * sw1[:, None]\n",
    "        yw1 = y1 * sw1\n",
    "        beta1 = np.linalg.lstsq(Xw1, yw1, rcond=None)[0]\n",
    "        return beta1\n",
    "\n",
    "    X_hat_list = []\n",
    "    for j in range(endog.shape[1]):\n",
    "        b1 = wls_simple(endog[:, j], Z, w)\n",
    "        X_hat_list.append(Z @ b1)\n",
    "    X_hat = np.column_stack(X_hat_list)\n",
    "\n",
    "    # -------------- Stage 2: WLS of y on [exog, fitted endog] --------------\n",
    "    X2 = np.column_stack([exog, X_hat])\n",
    "    names = exog_names + endog_names\n",
    "\n",
    "    beta2, vcov2 = wls_cluster(y, X2, w, clusters)\n",
    "    params = pd.Series(beta2, index=names)\n",
    "    ses = pd.Series(np.sqrt(np.diag(vcov2)), index=names)\n",
    "\n",
    "    return params, ses\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Load main CES dataset (ces_gdp_rct_v1a.dta)\n",
    "# ============================================================\n",
    "\n",
    "# Adjust path if needed\n",
    "stata_path = \"ces_gdp_rct_v1a.dta\"\n",
    "df, meta = pyreadstat.read_dta(stata_path)\n",
    "\n",
    "# Basic numeric coercions for key variables we know we'll use\n",
    "num_cols_basic = [\n",
    "    \"hid\", \"wave\", \"wgt\", \"cons_tot\", \"cons_dbt\",\n",
    "    \"egrea_imean_bt\", \"egrea_istd_bt\", \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\", \"treat5\",\n",
    "    \"age\", \"hhsize\", \"lhhnetinc\", \"liquid\",\n",
    "    \"male\", \"k1010_3\", \"pr2010\", \"pr2110\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "]\n",
    "to_numeric(df, num_cols_basic)\n",
    "\n",
    "# Some datasets already have these; if missing, leave as NA.\n",
    "if \"flag_speeder\" in df.columns:\n",
    "    df[\"flag_speeder\"] = pd.to_numeric(df[\"flag_speeder\"], errors=\"coerce\")\n",
    "else:\n",
    "    df[\"flag_speeder\"] = np.nan\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Build Huber weights (matching Huber_weight.dta creation)\n",
    "#    - drop treat5\n",
    "#    - keep wave==9\n",
    "#    - Huber robust RLM for egrea_imean_pt and egrea_istd_pt\n",
    "# ============================================================\n",
    "\n",
    "def build_huber_weights(df_in):\n",
    "    d = df_in.copy()\n",
    "\n",
    "    # Drop treatment 5 as in the Huber do-file\n",
    "    d = d[d[\"treat5\"] != 1].copy()\n",
    "\n",
    "    # Keep wave 9 only\n",
    "    d = d[d[\"wave\"] == 9].copy()\n",
    "\n",
    "    # ID\n",
    "    d[\"ID\"] = d[\"hid\"]\n",
    "\n",
    "    # Treatment dummies A-style\n",
    "    d[\"treat2A\"] = np.where(d[\"treat2\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat3\", \"treat4\"]].any(axis=1), \"treat2A\"] = 0\n",
    "\n",
    "    d[\"treat3A\"] = np.where(d[\"treat3\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat2\", \"treat4\"]].any(axis=1), \"treat3A\"] = 0\n",
    "\n",
    "    d[\"treat4A\"] = np.where(d[\"treat4\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat2\", \"treat3\"]].any(axis=1), \"treat4A\"] = 0\n",
    "\n",
    "    # xvar1 = egrea_imean, xvar2 = egrea_istd\n",
    "    # initi and interaction means by ID (here wave=9 only, but we keep by-ID structure)\n",
    "    for var in [\"egrea_imean\", \"egrea_istd\"]:\n",
    "        bt = f\"{var}_bt\"\n",
    "\n",
    "        d[f\"initi_{var}\"] = d.groupby(\"ID\")[bt].transform(\"mean\")\n",
    "\n",
    "        # treat2A\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat2A\"]\n",
    "        d[f\"initXi2_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        # treat3A\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat3A\"]\n",
    "        d[f\"initXi3_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        # treat4A\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat4A\"]\n",
    "        d[f\"initXi4_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        d.drop(columns=\"_tmp\", inplace=True)\n",
    "\n",
    "    # All numeric for RLM\n",
    "    rl_cols = [\n",
    "        \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "        \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "        \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "        \"initXi2_egrea_imean\", \"initXi3_egrea_imean\", \"initXi4_egrea_imean\",\n",
    "        \"initXi2_egrea_istd\", \"initXi3_egrea_istd\", \"initXi4_egrea_istd\",\n",
    "        \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "        \"wgt\"\n",
    "    ]\n",
    "    to_numeric(d, rl_cols)\n",
    "\n",
    "    # sqrt weights\n",
    "    d[\"_W\"] = np.sqrt(d[\"wgt\"])\n",
    "\n",
    "    # Construct RLM design columns\n",
    "    d[\"_Y1\"] = d[\"egrea_imean_pt\"] * d[\"_W\"]\n",
    "    d[\"_Y2\"] = d[\"egrea_istd_pt\"] * d[\"_W\"]\n",
    "\n",
    "    d[\"_X1\"] = d[\"initi_egrea_imean\"] * d[\"_W\"]\n",
    "    d[\"_X2\"] = d[\"initi_egrea_istd\"] * d[\"_W\"]\n",
    "\n",
    "    d[\"_M1\"] = d[\"treat2A\"] * d[\"_W\"]\n",
    "    d[\"_M2\"] = d[\"treat3A\"] * d[\"_W\"]\n",
    "    d[\"_M3\"] = d[\"treat4A\"] * d[\"_W\"]\n",
    "\n",
    "    d[\"_Z1\"] = d[\"initXi2_egrea_imean\"] * d[\"_W\"]\n",
    "    d[\"_Z2\"] = d[\"initXi3_egrea_imean\"] * d[\"_W\"]\n",
    "    d[\"_Z3\"] = d[\"initXi2_egrea_istd\"] * d[\"_W\"]\n",
    "    d[\"_Z4\"] = d[\"initXi3_egrea_istd\"] * d[\"_W\"]\n",
    "    d[\"_Z5\"] = d[\"initXi4_egrea_imean\"] * d[\"_W\"]\n",
    "    d[\"_Z6\"] = d[\"initXi4_egrea_istd\"] * d[\"_W\"]\n",
    "\n",
    "    for c in [\"_cnt1\", \"_cnt2\", \"_cnt3\", \"_cnt4\", \"_cnt5\", \"_cnt6\"]:\n",
    "        d[c] = np.nan\n",
    "    d[\"_cnt1\"] = d[\"cnt1\"] * d[\"_W\"]\n",
    "    d[\"_cnt2\"] = d[\"cnt2\"] * d[\"_W\"]\n",
    "    d[\"_cnt3\"] = d[\"cnt3\"] * d[\"_W\"]\n",
    "    d[\"_cnt4\"] = d[\"cnt4\"] * d[\"_W\"]\n",
    "    d[\"_cnt5\"] = d[\"cnt5\"] * d[\"_W\"]\n",
    "    d[\"_cnt6\"] = d[\"cnt6\"] * d[\"_W\"]\n",
    "\n",
    "    # Clean NA rows for RLM\n",
    "    rlm_cols = [\n",
    "        \"_Y1\", \"_Y2\", \"_X1\", \"_X2\", \"_W\",\n",
    "        \"_M1\", \"_M2\", \"_M3\",\n",
    "        \"_Z1\", \"_Z2\", \"_Z3\", \"_Z4\", \"_Z5\", \"_Z6\",\n",
    "        \"_cnt1\", \"_cnt2\", \"_cnt3\", \"_cnt4\", \"_cnt5\", \"_cnt6\"\n",
    "    ]\n",
    "    d_rlm = d.dropna(subset=rlm_cols).copy()\n",
    "\n",
    "    X_rlm = d_rlm[[\"_X1\", \"_X2\", \"_W\", \"_M1\", \"_M2\", \"_M3\",\n",
    "                   \"_Z1\", \"_Z2\", \"_Z3\", \"_Z4\", \"_Z5\", \"_Z6\",\n",
    "                   \"_cnt1\", \"_cnt2\", \"_cnt3\", \"_cnt4\", \"_cnt5\", \"_cnt6\"]]\n",
    "    X_rlm = sm.add_constant(X_rlm)\n",
    "\n",
    "    # RLM for mean\n",
    "    rlm1 = sm.RLM(d_rlm[\"_Y1\"], X_rlm, M=HuberT())\n",
    "    res1 = rlm1.fit()\n",
    "    d[\"_wgt1\"] = np.nan\n",
    "    d.loc[d_rlm.index, \"_wgt1\"] = res1.weights\n",
    "\n",
    "    # RLM for stdev\n",
    "    rlm2 = sm.RLM(d_rlm[\"_Y2\"], X_rlm, M=HuberT())\n",
    "    res2 = rlm2.fit()\n",
    "    d[\"_wgt2\"] = np.nan\n",
    "    d.loc[d_rlm.index, \"_wgt2\"] = res2.weights\n",
    "\n",
    "    # Final Huber weight\n",
    "    d[\"_wgt\"] = np.sqrt(d[\"_wgt1\"].fillna(0.0)) * np.sqrt(d[\"_wgt2\"].fillna(0.0)) * d[\"wgt\"]\n",
    "\n",
    "    huber = d[[\"hid\", \"_wgt\"]].copy()\n",
    "    huber = huber.dropna(subset=[\"_wgt\"])\n",
    "\n",
    "    return huber\n",
    "\n",
    "\n",
    "huber_weights = build_huber_weights(df)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build Table-3 regression sample\n",
    "#    - log(f1(cons_tot - cons_dbt)) at wave 9\n",
    "#    - drop speeders\n",
    "#    - drop extreme lc (5–10)\n",
    "#    - drop treat5\n",
    "#    - build initi_xvar and interaction instruments\n",
    "#    - merge Huber _wgt\n",
    "# ============================================================\n",
    "\n",
    "def build_reg_sample(df_in, huber):\n",
    "    d = df_in.copy()\n",
    "\n",
    "    # ID\n",
    "    d[\"ID\"] = d[\"hid\"]\n",
    "\n",
    "    # Sort and build net_cons_lead = f1(cons_tot - cons_dbt)\n",
    "    d = d.sort_values([\"ID\", \"wave\"])\n",
    "    d[\"net_cons\"] = d[\"cons_tot\"] - d[\"cons_dbt\"]\n",
    "    d[\"net_cons_lead\"] = d.groupby(\"ID\")[\"net_cons\"].shift(-1)\n",
    "\n",
    "    # lc = log(f1(cons_tot - cons_dbt)) only relevant at wave 9\n",
    "    d[\"lc\"] = np.log(d[\"net_cons_lead\"])\n",
    "\n",
    "    # drop extreme lc at wave 9\n",
    "    d.loc[(d[\"wave\"] == 9) & (d[\"lc\"] < 5), \"lc\"] = np.nan\n",
    "    d.loc[(d[\"wave\"] == 9) & (d[\"lc\"] > 10), \"lc\"] = np.nan\n",
    "\n",
    "    # Drop speeders if flag exists\n",
    "    d = d[d[\"flag_speeder\"] != 1]\n",
    "\n",
    "    # Drop treatment 5 as paper notes\n",
    "    d = d[d[\"treat5\"] != 1].copy()\n",
    "\n",
    "    # A-style treatments\n",
    "    d[\"treat2A\"] = np.where(d[\"treat2\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat3\", \"treat4\"]].any(axis=1), \"treat2A\"] = 0\n",
    "\n",
    "    d[\"treat3A\"] = np.where(d[\"treat3\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat2\", \"treat4\"]].any(axis=1), \"treat3A\"] = 0\n",
    "\n",
    "    d[\"treat4A\"] = np.where(d[\"treat4\"] == 1, 1, 0)\n",
    "    d.loc[d[[\"treat1\", \"treat2\", \"treat3\"]].any(axis=1), \"treat4A\"] = 0\n",
    "\n",
    "    # Age^2/100, pr2010D, pr2110D\n",
    "    d[\"age2\"] = (d[\"age\"] ** 2) / 100.0\n",
    "    d[\"pr2010D\"] = np.where(d[\"pr2010\"] == 2, 1, 0)\n",
    "    d[\"pr2110D\"] = np.where(d[\"pr2110\"] == 2, 1, 0)\n",
    "\n",
    "    # Keep only wave 9 (as in do-file)\n",
    "    d = d[d[\"wave\"] == 9].copy()\n",
    "\n",
    "    # initi and interaction instruments for xvar1=egrea_imean, xvar2=egrea_istd\n",
    "    for var in [\"egrea_imean\", \"egrea_istd\"]:\n",
    "        bt = f\"{var}_bt\"\n",
    "\n",
    "        d[f\"initi_{var}\"] = d.groupby(\"ID\")[bt].transform(\"mean\")\n",
    "\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat2A\"]\n",
    "        d[f\"initXi2_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat3A\"]\n",
    "        d[f\"initXi3_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        d[\"_tmp\"] = d[bt] * d[\"treat4A\"]\n",
    "        d[f\"initXi4_{var}\"] = d.groupby(\"ID\")[\"_tmp\"].transform(\"mean\")\n",
    "\n",
    "        d.drop(columns=\"_tmp\", inplace=True)\n",
    "\n",
    "    # Merge Huber weights (by hid)\n",
    "    d = d.merge(huber, on=\"hid\", how=\"left\")\n",
    "\n",
    "    # Make sure all needed cols are numeric\n",
    "    cols_needed = [\n",
    "        \"lc\", \"_wgt\", \"ID\",\n",
    "        \"dedu2\", \"dedu3\", \"age\", \"age2\", \"hhsize\",\n",
    "        \"lhhnetinc\", \"liquid\", \"male\", \"k1010_3\",\n",
    "        \"pr2010D\", \"pr2110D\",\n",
    "        \"cnt1\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "        \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "        \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "        \"initXi2_egrea_imean\", \"initXi3_egrea_imean\", \"initXi4_egrea_imean\",\n",
    "        \"initXi2_egrea_istd\", \"initXi3_egrea_istd\", \"initXi4_egrea_istd\",\n",
    "        \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "    ]\n",
    "    to_numeric(d, cols_needed)\n",
    "\n",
    "    # Drop rows with any NA in required vars\n",
    "    d = d.dropna(subset=cols_needed).copy()\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "df_reg = build_reg_sample(df, huber_weights)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Run IV 2SLS (full sample, no trimming)\n",
    "# ============================================================\n",
    "\n",
    "def run_iv_2sls(df_in, y_col):\n",
    "    d = df_in.copy()\n",
    "\n",
    "    # spec0\n",
    "    spec0 = [\n",
    "        \"dedu2\", \"dedu3\",\n",
    "        \"age\", \"age2\",\n",
    "        \"hhsize\",\n",
    "        \"lhhnetinc\",\n",
    "        \"liquid\",\n",
    "        \"male\",\n",
    "        \"k1010_3\",\n",
    "        \"pr2010D\", \"pr2110D\",\n",
    "    ]\n",
    "\n",
    "    # country dummies (note: NO cnt2, matches Stata global cnt)\n",
    "    cnt = [\"cnt1\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]\n",
    "\n",
    "    exog_cols = spec0 + [\"initi_egrea_imean\", \"initi_egrea_istd\"] + cnt\n",
    "    endog_cols = [\"egrea_imean_pt\", \"egrea_istd_pt\"]\n",
    "    instr_cols = [\n",
    "        \"initXi2_egrea_imean\",\n",
    "        \"initXi3_egrea_imean\",\n",
    "        \"initXi4_egrea_imean\",\n",
    "        \"treat2A\",\n",
    "        \"initXi2_egrea_istd\",\n",
    "        \"initXi3_egrea_istd\",\n",
    "        \"initXi4_egrea_istd\",\n",
    "        \"treat3A\",\n",
    "        \"treat4A\",\n",
    "    ]\n",
    "\n",
    "    used = [y_col, \"_wgt\", \"ID\"] + exog_cols + endog_cols + instr_cols\n",
    "    to_numeric(d, used)\n",
    "    d = d.dropna(subset=used).copy()\n",
    "\n",
    "    y = d[y_col].values\n",
    "    exog = d[exog_cols].values\n",
    "    exog = np.column_stack([np.ones(len(d)), exog])  # add constant\n",
    "    exog_names = [\"const\"] + exog_cols\n",
    "\n",
    "    endog = d[endog_cols].values\n",
    "    instr = d[instr_cols].values\n",
    "\n",
    "    w = d[\"_wgt\"].values\n",
    "    clusters = d[\"ID\"].values\n",
    "\n",
    "    params, ses = iv_2sls_cluster(\n",
    "        y, exog, endog, instr, w, clusters, exog_names, endog_cols\n",
    "    )\n",
    "\n",
    "    return params, ses\n",
    "\n",
    "\n",
    "# Full-sample IV on lc (unscaled) for jackknife normalization\n",
    "full_params, full_ses = run_iv_2sls(df_reg, y_col=\"lc\")\n",
    "\n",
    "b1_full = full_params[\"egrea_imean_pt\"]\n",
    "se1_full = full_ses[\"egrea_imean_pt\"]\n",
    "b2_full = full_params[\"egrea_istd_pt\"]\n",
    "se2_full = full_ses[\"egrea_istd_pt\"]\n",
    "\n",
    "print(\"Full-sample IV (no trimming, DV = lc):\")\n",
    "print(\" First_Moment_Post (egrea_imean_pt):  beta =\", b1_full, \"  se =\", se1_full)\n",
    "print(\" Second_Moment_Post (egrea_istd_pt): beta =\", b2_full, \"  se =\", se2_full)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) Jackknife: delete-1 to get _c_lc1, _c_lc2 and normalized versions\n",
    "# ============================================================\n",
    "\n",
    "N = df_reg.shape[0]\n",
    "c1_jk = np.empty(N)\n",
    "c2_jk = np.empty(N)\n",
    "\n",
    "df_reg = df_reg.reset_index(drop=True)\n",
    "\n",
    "for i in range(N):\n",
    "    df_j = df_reg.drop(index=i).copy()\n",
    "    p_j, _ = run_iv_2sls(df_j, y_col=\"lc\")\n",
    "    c1_jk[i] = p_j[\"egrea_imean_pt\"]\n",
    "    c2_jk[i] = p_j[\"egrea_istd_pt\"]\n",
    "\n",
    "df_reg[\"_c_lc1\"] = c1_jk\n",
    "df_reg[\"_c_lc2\"] = c2_jk\n",
    "\n",
    "df_reg[\"_c_lc1_norm\"] = (df_reg[\"_c_lc1\"] - b1_full) / se1_full\n",
    "df_reg[\"_c_lc2_norm\"] = (df_reg[\"_c_lc2\"] - b2_full) / se2_full\n",
    "\n",
    "# Scale lc by 100 for final regression as in Stata: replace lc = lc*100\n",
    "df_reg[\"lc_100\"] = df_reg[\"lc\"] * 100.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) Final IV 2SLS with jackknife trimming\n",
    "#    if (abs(_c_lc1_norm) + abs(_c_lc2_norm)) < 0.25\n",
    "# ============================================================\n",
    "\n",
    "trim_mask = (df_reg[\"_c_lc1_norm\"].abs() + df_reg[\"_c_lc2_norm\"].abs()) < 0.25\n",
    "df_trim = df_reg.loc[trim_mask].copy()\n",
    "\n",
    "final_params, final_ses = run_iv_2sls(df_trim, y_col=\"lc_100\")\n",
    "\n",
    "b1_final = final_params[\"egrea_imean_pt\"]\n",
    "se1_final = final_ses[\"egrea_imean_pt\"]\n",
    "b2_final = final_params[\"egrea_istd_pt\"]\n",
    "se2_final = final_ses[\"egrea_istd_pt\"]\n",
    "\n",
    "print(\"\\nFinal IV (with Huber weights + jackknife trimming, DV = lc*100):\")\n",
    "print(\" First_Moment_Post (egrea_imean_pt):\")\n",
    "print(\"    beta =\", b1_final)\n",
    "print(\"    se   =\", se1_final)\n",
    "\n",
    "print(\" Second_Moment_Post (egrea_istd_pt):\")\n",
    "print(\"    beta =\", b2_final)\n",
    "print(\"    se   =\", se2_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14a3d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined [exog | instr] columns: 28   rank: 28\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:              lc_scaled   R-squared:                      0.1803\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1776\n",
      "No. Observations:                5906   F-statistic:                    1027.6\n",
      "Date:                Mon, Nov 17 2025   P-value (F-stat)                0.0000\n",
      "Time:                        21:00:21   Distribution:                 chi2(20)\n",
      "Cov. Estimator:             clustered                                         \n",
      "                                                                              \n",
      "                                 Parameter Estimates                                 \n",
      "=====================================================================================\n",
      "                   Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 576.44     14.570     39.563     0.0000      547.88      604.99\n",
      "dedu2                 0.8023     2.8447     0.2820     0.7779     -4.7732      6.3779\n",
      "dedu3                 12.068     2.7671     4.3611     0.0000      6.6442      17.491\n",
      "age                   1.7371     0.3434     5.0592     0.0000      1.0641      2.4100\n",
      "age2                 -1.2914     0.3536    -3.6523     0.0003     -1.9845     -0.5984\n",
      "hhsize                10.890     0.7156     15.218     0.0000      9.4875      12.293\n",
      "lhhnetinc             8.3028     1.0724     7.7422     0.0000      6.2010      10.405\n",
      "liquid                14.561     2.1712     6.7064     0.0000      10.305      18.816\n",
      "male                  4.5683     1.8255     2.5025     0.0123      0.9904      8.1462\n",
      "k1010_3               6.2844     3.5170     1.7869     0.0740     -0.6088      13.178\n",
      "pr2010D              -19.473     2.2099    -8.8116     0.0000     -23.804     -15.141\n",
      "pr2110D               1.0432     2.4222     0.4307     0.6667     -3.7042      5.7906\n",
      "initi_egrea_imean    -0.2015     0.2247    -0.8964     0.3700     -0.6420      0.2390\n",
      "initi_egrea_istd      3.2039     0.9657     3.3177     0.0009      1.3111      5.0966\n",
      "cnt1                 -19.219     3.3200    -5.7888     0.0000     -25.726     -12.712\n",
      "cnt3                 -39.235     2.6768    -14.658     0.0000     -44.481     -33.989\n",
      "cnt4                 -23.750     2.4727    -9.6052     0.0000     -28.597     -18.904\n",
      "cnt5                 -25.032     2.7911    -8.9685     0.0000     -30.502     -19.562\n",
      "cnt6                 -4.1312     3.4048    -1.2133     0.2250     -10.804      2.5421\n",
      "egrea_imean_pt       -0.1026     0.9187    -0.1117     0.9111     -1.9033      1.6981\n",
      "egrea_istd_pt        -5.5589     2.1307    -2.6089     0.0091     -9.7350     -1.3827\n",
      "=====================================================================================\n",
      "\n",
      "Endogenous: egrea_imean_pt, egrea_istd_pt\n",
      "Instruments: initXi2_egrea_imean, initXi3_egrea_imean, initXi4_egrea_imean, initXi2_egrea_istd, initXi3_egrea_istd, initXi4_egrea_istd, treat2A, treat3A, treat4A\n",
      "Clustered Covariance (One-Way)\n",
      "Debiased: False\n",
      "Num Clusters: 5906\n",
      "\n",
      "--- Key coefficients (DV: 100 * log net consumption) ---\n",
      "First_Moment_Post (egrea_imean_pt):  beta = -0.1026,  se = 0.9187\n",
      "Second_Moment_Post (egrea_istd_pt):  beta = -5.5589,  se = 2.1307\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper: drop constant columns (including all-zero) except 'const'\n",
    "# ---------------------------------------------------------------------\n",
    "def drop_constant_columns(df, label=\"\"):\n",
    "    keep = []\n",
    "    for c in df.columns:\n",
    "        if c == \"const\":\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if s.nunique(dropna=True) > 1:\n",
    "            keep.append(c)\n",
    "    dropped = [c for c in df.columns if c not in keep]\n",
    "    if dropped:\n",
    "        print(f\"[{label}] Dropped constant/all-zero columns:\", dropped)\n",
    "    return df[keep]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Load CES data\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_stata(\"ces_gdp_rct_v1a.dta\", convert_categoricals=False)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Basic restrictions and numeric coercion\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Drop treatment 5 (country growth 2nd moment) as in the paper\n",
    "if \"treat5\" in df.columns:\n",
    "    df = df[df[\"treat5\"] != 1]\n",
    "\n",
    "# Drop speeders if available\n",
    "if \"flag_speeder\" in df.columns:\n",
    "    df = df[df[\"flag_speeder\"] != 1]\n",
    "\n",
    "# Keep only waves 7–16 (matching prep do-file)\n",
    "df = df[df[\"wave\"].between(7, 16)]\n",
    "\n",
    "# Coerce key vars to numeric\n",
    "num_vars_basic = [\n",
    "    \"wave\", \"hid\", \"wgt\",\n",
    "    \"cons_tot\", \"cons_dbt\",\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"hhsize\", \"lhhnetinc\",\n",
    "    \"liquid\", \"male\", \"k1010_3\", \"pr2010\", \"pr2110\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\"\n",
    "]\n",
    "for v in num_vars_basic:\n",
    "    if v in df.columns:\n",
    "        df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Treatment dummies and controls\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df[\"treat2A\"] = np.where(df[\"treat2\"] == 1, 1.0, 0.0)\n",
    "df[\"treat3A\"] = np.where(df[\"treat3\"] == 1, 1.0, 0.0)\n",
    "df[\"treat4A\"] = np.where(df[\"treat4\"] == 1, 1.0, 0.0)\n",
    "\n",
    "df[\"age2\"] = (df[\"age\"] ** 2) / 100.0\n",
    "df[\"pr2010D\"] = np.where(df[\"pr2010\"] == 2, 1.0, 0.0)\n",
    "df[\"pr2110D\"] = np.where(df[\"pr2110\"] == 2, 1.0, 0.0)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Future consumption and log cons (lc)\n",
    "#    lc = log(f1.cons_tot - f1.cons_dbt) if wave == 9\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df = df.sort_values([\"hid\", \"wave\"])\n",
    "\n",
    "df[\"cons_tot_lead\"] = df.groupby(\"hid\")[\"cons_tot\"].shift(-1)\n",
    "df[\"cons_dbt_lead\"] = df.groupby(\"hid\")[\"cons_dbt\"].shift(-1)\n",
    "df[\"net_cons_lead\"] = df[\"cons_tot_lead\"] - df[\"cons_dbt_lead\"]\n",
    "\n",
    "for col in [\"cons_tot_lead\", \"cons_dbt_lead\", \"net_cons_lead\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# net consumption must be positive for log\n",
    "df.loc[df[\"net_cons_lead\"] <= 0, \"net_cons_lead\"] = np.nan\n",
    "\n",
    "df[\"lc\"] = np.where(df[\"wave\"] == 9, np.log(df[\"net_cons_lead\"]), np.nan)\n",
    "\n",
    "# Drop extreme lc\n",
    "mask_w9 = df[\"wave\"] == 9\n",
    "df.loc[mask_w9 & (df[\"lc\"] < 5), \"lc\"] = np.nan\n",
    "df.loc[mask_w9 & (df[\"lc\"] > 10), \"lc\"] = np.nan\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Wave 9 subset and initi_* variables\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df_w9 = df[df[\"wave\"] == 9].copy()\n",
    "\n",
    "# initi_ egrea_imean / egrea_istd\n",
    "df_w9[\"_initi_egrea_imean\"] = df_w9[\"egrea_imean_bt\"]\n",
    "df_w9[\"initi_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initi_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initi_egrea_istd\"] = df_w9[\"egrea_istd_bt\"]\n",
    "df_w9[\"initi_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initi_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "# Interactions for egrea_imean\n",
    "df_w9[\"_initXi2_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat2A\"]\n",
    "df_w9[\"initXi2_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi2_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi3_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat3A\"]\n",
    "df_w9[\"initXi3_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi3_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi4_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat4A\"]\n",
    "df_w9[\"initXi4_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi4_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "# Interactions for egrea_istd\n",
    "df_w9[\"_initXi2_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat2A\"]\n",
    "df_w9[\"initXi2_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi2_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi3_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat3A\"]\n",
    "df_w9[\"initXi3_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi3_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi4_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat4A\"]\n",
    "df_w9[\"initXi4_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi4_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Huber robust first stage (approx rreg2 logic)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "sqrtw = np.sqrt(df_w9[\"wgt\"])\n",
    "\n",
    "Y1 = df_w9[\"egrea_imean_pt\"] * sqrtw\n",
    "Y2 = df_w9[\"egrea_istd_pt\"] * sqrtw\n",
    "\n",
    "X1 = df_w9[\"initi_egrea_imean\"] * sqrtw\n",
    "X2 = df_w9[\"initi_egrea_istd\"] * sqrtw\n",
    "\n",
    "M1 = df_w9[\"treat2A\"] * sqrtw\n",
    "M2 = df_w9[\"treat3A\"] * sqrtw\n",
    "M3 = df_w9[\"treat4A\"] * sqrtw\n",
    "\n",
    "Z1 = df_w9[\"initXi2_egrea_imean\"] * sqrtw\n",
    "Z2 = df_w9[\"initXi3_egrea_imean\"] * sqrtw\n",
    "Z3 = df_w9[\"initXi2_egrea_istd\"] * sqrtw\n",
    "Z4 = df_w9[\"initXi3_egrea_istd\"] * sqrtw\n",
    "Z5 = df_w9[\"initXi4_egrea_imean\"] * sqrtw\n",
    "Z6 = df_w9[\"initXi4_egrea_istd\"] * sqrtw\n",
    "\n",
    "X_h = pd.DataFrame({\n",
    "    \"const\": 1.0,\n",
    "    \"X1\": X1,\n",
    "    \"X2\": X2,\n",
    "    \"W\": sqrtw,\n",
    "    \"M1\": M1,\n",
    "    \"M2\": M2,\n",
    "    \"M3\": M3,\n",
    "    \"Z1\": Z1,\n",
    "    \"Z2\": Z2,\n",
    "    \"Z3\": Z3,\n",
    "    \"Z4\": Z4,\n",
    "    \"Z5\": Z5,\n",
    "    \"Z6\": Z6,\n",
    "})\n",
    "\n",
    "# Add country dummies (all; redundancy handled later)\n",
    "for c in [\"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]:\n",
    "    if c in df_w9.columns:\n",
    "        X_h[c] = df_w9[c] * sqrtw\n",
    "\n",
    "# Clean finite rows\n",
    "mask_finite = np.isfinite(Y1) & np.isfinite(Y2)\n",
    "for col in X_h.columns:\n",
    "    mask_finite &= np.isfinite(X_h[col])\n",
    "\n",
    "X_h_clean = X_h.loc[mask_finite]\n",
    "Y1_clean = Y1.loc[mask_finite]\n",
    "Y2_clean = Y2.loc[mask_finite]\n",
    "\n",
    "# RLM with Huber T for both equations\n",
    "rlm1 = sm.RLM(Y1_clean, X_h_clean, M=HuberT())\n",
    "res1 = rlm1.fit()\n",
    "\n",
    "rlm2 = sm.RLM(Y2_clean, X_h_clean, M=HuberT())\n",
    "res2 = rlm2.fit()\n",
    "\n",
    "# Extract weights back into full df_w9 index\n",
    "w1 = pd.Series(1.0, index=df_w9.index)\n",
    "w2 = pd.Series(1.0, index=df_w9.index)\n",
    "w1.loc[mask_finite] = res1.weights\n",
    "w2.loc[mask_finite] = res2.weights\n",
    "\n",
    "df_w9[\"_wgt1\"] = w1\n",
    "df_w9[\"_wgt2\"] = w2\n",
    "df_w9[\"huber_wgt\"] = np.sqrt(df_w9[\"_wgt1\"]) * np.sqrt(df_w9[\"_wgt2\"]) * df_w9[\"wgt\"]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) Build 2SLS design\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "spec0_vars = [\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"age2\", \"hhsize\",\n",
    "    \"lhhnetinc\", \"liquid\", \"male\", \"k1010_3\",\n",
    "    \"pr2010D\", \"pr2110D\"\n",
    "]\n",
    "cnt_vars = [\"cnt1\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]  # cnt2 omitted as base\n",
    "\n",
    "needed_for_reg = (\n",
    "    [\"lc\", \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "     \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "     \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "     \"huber_wgt\", \"hid\"]\n",
    "    + spec0_vars\n",
    "    + cnt_vars\n",
    "    + [\n",
    "        \"initXi2_egrea_imean\", \"initXi3_egrea_imean\", \"initXi4_egrea_imean\",\n",
    "        \"initXi2_egrea_istd\", \"initXi3_egrea_istd\", \"initXi4_egrea_istd\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for v in needed_for_reg:\n",
    "    if v in df_w9.columns:\n",
    "        df_w9[v] = pd.to_numeric(df_w9[v], errors=\"coerce\")\n",
    "\n",
    "df_reg = df_w9.dropna(subset=[\n",
    "    \"lc\", \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "    \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "    \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "    \"huber_wgt\"\n",
    "] + spec0_vars)\n",
    "\n",
    "# Scale lc by 100 (as in Stata normalize step)\n",
    "df_reg = df_reg.copy()\n",
    "df_reg[\"lc_scaled\"] = df_reg[\"lc\"] * 100.0\n",
    "\n",
    "# Endogenous\n",
    "endog = df_reg[[\"egrea_imean_pt\", \"egrea_istd_pt\"]]\n",
    "\n",
    "# Exogenous (NO treat dummies here; those are only instruments)\n",
    "exog = pd.DataFrame({\"const\": 1.0}, index=df_reg.index)\n",
    "for v in spec0_vars:\n",
    "    exog[v] = df_reg[v]\n",
    "exog[\"initi_egrea_imean\"] = df_reg[\"initi_egrea_imean\"]\n",
    "exog[\"initi_egrea_istd\"] = df_reg[\"initi_egrea_istd\"]\n",
    "for v in cnt_vars:\n",
    "    if v in df_reg.columns:\n",
    "        exog[v] = df_reg[v]\n",
    "\n",
    "# Instruments – treat dummies + interactions\n",
    "instr = pd.DataFrame(index=df_reg.index)\n",
    "instr[\"initXi2_egrea_imean\"] = df_reg[\"initXi2_egrea_imean\"]\n",
    "instr[\"initXi3_egrea_imean\"] = df_reg[\"initXi3_egrea_imean\"]\n",
    "instr[\"initXi4_egrea_imean\"] = df_reg[\"initXi4_egrea_imean\"]\n",
    "instr[\"initXi2_egrea_istd\"] = df_reg[\"initXi2_egrea_istd\"]\n",
    "instr[\"initXi3_egrea_istd\"] = df_reg[\"initXi3_egrea_istd\"]\n",
    "instr[\"initXi4_egrea_istd\"] = df_reg[\"initXi4_egrea_istd\"]\n",
    "instr[\"treat2A\"] = df_reg[\"treat2A\"]\n",
    "instr[\"treat3A\"] = df_reg[\"treat3A\"]\n",
    "instr[\"treat4A\"] = df_reg[\"treat4A\"]\n",
    "\n",
    "# Drop constant columns from exog and instr to avoid rank deficiency\n",
    "exog = drop_constant_columns(exog, label=\"exog\")\n",
    "instr = drop_constant_columns(instr, label=\"instr\")\n",
    "\n",
    "# Dep var, weights, clusters\n",
    "y = df_reg[\"lc_scaled\"]\n",
    "weights = df_reg[\"huber_wgt\"]\n",
    "clusters = df_reg[\"hid\"]\n",
    "\n",
    "# Quick diagnostic: matrix rank\n",
    "Z = pd.concat([exog, instr], axis=1)\n",
    "rank_Z = np.linalg.matrix_rank(Z.values)\n",
    "print(\"Combined [exog | instr] columns:\", Z.shape[1], \"  rank:\", rank_Z)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) Run IV 2SLS with Huber weights & clustered SE\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "iv_mod = IV2SLS(\n",
    "    dependent=y,\n",
    "    exog=exog,\n",
    "    endog=endog,\n",
    "    instruments=instr,\n",
    "    weights=weights  # <-- weights belong here\n",
    ")\n",
    "\n",
    "res = iv_mod.fit(\n",
    "    cov_type=\"clustered\",\n",
    "    clusters=clusters\n",
    ")\n",
    "\n",
    "print(res.summary)\n",
    "\n",
    "# Key coefficients\n",
    "beta_mean = res.params[\"egrea_imean_pt\"]\n",
    "se_mean = res.std_errors[\"egrea_imean_pt\"]\n",
    "\n",
    "beta_unc = res.params[\"egrea_istd_pt\"]\n",
    "se_unc = res.std_errors[\"egrea_istd_pt\"]\n",
    "\n",
    "print(\"\\n--- Key coefficients (DV: 100 * log net consumption) ---\")\n",
    "print(f\"First_Moment_Post (egrea_imean_pt):  beta = {beta_mean:.4f},  se = {se_mean:.4f}\")\n",
    "print(f\"Second_Moment_Post (egrea_istd_pt):  beta = {beta_unc:.4f},  se = {se_unc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ec92185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined [exog | instr] columns: 28   rank: 28\n",
      "                          IV-2SLS Estimation Summary                          \n",
      "==============================================================================\n",
      "Dep. Variable:              lc_scaled   R-squared:                      0.1794\n",
      "Estimator:                    IV-2SLS   Adj. R-squared:                 0.1765\n",
      "No. Observations:                5906   F-statistic:                    1033.5\n",
      "Date:                Mon, Nov 17 2025   P-value (F-stat)                0.0000\n",
      "Time:                        21:16:07   Distribution:                 chi2(21)\n",
      "Cov. Estimator:             clustered                                         \n",
      "                                                                              \n",
      "                                  Parameter Estimates                                  \n",
      "=======================================================================================\n",
      "                     Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   576.74     14.599     39.506     0.0000      548.13      605.36\n",
      "dedu2                   0.8111     2.8464     0.2849     0.7757     -4.7678      6.3900\n",
      "dedu3                   12.050     2.7693     4.3511     0.0000      6.6219      17.477\n",
      "age                     1.7345     0.3436     5.0486     0.0000      1.0611      2.4079\n",
      "age2                   -1.2900     0.3537    -3.6474     0.0003     -1.9832     -0.5968\n",
      "hhsize                  10.888     0.7153     15.221     0.0000      9.4860      12.290\n",
      "lhhnetinc               8.2998     1.0724     7.7396     0.0000      6.1980      10.402\n",
      "liquid                  14.573     2.1733     6.7058     0.0000      10.314      18.833\n",
      "male                    4.5580     1.8254     2.4970     0.0125      0.9804      8.1357\n",
      "k1010_3                 6.3061     3.5178     1.7926     0.0730     -0.5887      13.201\n",
      "pr2010D                -19.485     2.2132    -8.8040     0.0000     -23.823     -15.147\n",
      "pr2110D                 1.0642     2.4243     0.4390     0.6607     -3.6874      5.8157\n",
      "initi_egrea_imean      -0.1956     0.2263    -0.8643     0.3874     -0.6392      0.2480\n",
      "initi_egrea_istd        3.3018     1.0331     3.1959     0.0014      1.2769      5.3266\n",
      "cnt1                   -19.205     3.3220    -5.7810     0.0000     -25.716     -12.693\n",
      "cnt3                   -39.240     2.6772    -14.657     0.0000     -44.487     -33.993\n",
      "cnt4                   -23.763     2.4765    -9.5955     0.0000     -28.617     -18.909\n",
      "cnt5                   -25.018     2.7908    -8.9644     0.0000     -30.488     -19.548\n",
      "cnt6                   -4.1565     3.4044    -1.2209     0.2221     -10.829      2.5160\n",
      "prior_uncert_treat3    -0.2092     0.6895    -0.3034     0.7615     -1.5607      1.1422\n",
      "egrea_imean_pt         -0.1309     0.9268    -0.1413     0.8876     -1.9475      1.6856\n",
      "egrea_istd_pt          -5.6570     2.1648    -2.6132     0.0090     -9.9000     -1.4141\n",
      "=======================================================================================\n",
      "\n",
      "Endogenous: egrea_imean_pt, egrea_istd_pt\n",
      "Instruments: initXi2_egrea_imean, initXi3_egrea_imean, initXi4_egrea_imean, initXi2_egrea_istd, initXi4_egrea_istd, treat2A, treat3A, treat4A\n",
      "Clustered Covariance (One-Way)\n",
      "Debiased: False\n",
      "Num Clusters: 5906\n",
      "\n",
      "--- Key coefficients (DV: 100 * log net consumption) ---\n",
      "First_Moment_Post (egrea_imean_pt):  beta = -0.1309,  se = 0.9268\n",
      "Second_Moment_Post (egrea_istd_pt):  beta = -5.6570,  se = 2.1648\n",
      "\n",
      "--- Interaction: prior uncertainty × Treatment 3 (initXi3_egrea_istd) ---\n",
      "beta = -0.2092,  se = 0.6895\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "from linearmodels.iv import IV2SLS\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper: drop constant columns (including all-zero) except 'const'\n",
    "# ---------------------------------------------------------------------\n",
    "def drop_constant_columns(df, label=\"\"):\n",
    "    keep = []\n",
    "    for c in df.columns:\n",
    "        if c == \"const\":\n",
    "            keep.append(c)\n",
    "            continue\n",
    "        s = df[c]\n",
    "        if s.nunique(dropna=True) > 1:\n",
    "            keep.append(c)\n",
    "    dropped = [c for c in df.columns if c not in keep]\n",
    "    if dropped:\n",
    "        print(f\"[{label}] Dropped constant/all-zero columns:\", dropped)\n",
    "    return df[keep]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1) Load CES data\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_stata(\"ces_gdp_rct_v1a.dta\", convert_categoricals=False)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Basic restrictions and numeric coercion\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Drop treatment 5 (country growth 2nd moment) as in the paper\n",
    "if \"treat5\" in df.columns:\n",
    "    df = df[df[\"treat5\"] != 1]\n",
    "\n",
    "# Drop speeders if available\n",
    "if \"flag_speeder\" in df.columns:\n",
    "    df = df[df[\"flag_speeder\"] != 1]\n",
    "\n",
    "# Keep only waves 7–16 (matching prep do-file)\n",
    "df = df[df[\"wave\"].between(7, 16)]\n",
    "\n",
    "# Coerce key vars to numeric\n",
    "num_vars_basic = [\n",
    "    \"wave\", \"hid\", \"wgt\",\n",
    "    \"cons_tot\", \"cons_dbt\",\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"hhsize\", \"lhhnetinc\",\n",
    "    \"liquid\", \"male\", \"k1010_3\", \"pr2010\", \"pr2110\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\"\n",
    "]\n",
    "for v in num_vars_basic:\n",
    "    if v in df.columns:\n",
    "        df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Treatment dummies and controls\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df[\"treat2A\"] = np.where(df[\"treat2\"] == 1, 1.0, 0.0)\n",
    "df[\"treat3A\"] = np.where(df[\"treat3\"] == 1, 1.0, 0.0)\n",
    "df[\"treat4A\"] = np.where(df[\"treat4\"] == 1, 1.0, 0.0)\n",
    "\n",
    "df[\"age2\"] = (df[\"age\"] ** 2) / 100.0\n",
    "df[\"pr2010D\"] = np.where(df[\"pr2010\"] == 2, 1.0, 0.0)\n",
    "df[\"pr2110D\"] = np.where(df[\"pr2110\"] == 2, 1.0, 0.0)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Future consumption and log cons (lc)\n",
    "#    lc = log(f1.cons_tot - f1.cons_dbt) if wave == 9\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df = df.sort_values([\"hid\", \"wave\"])\n",
    "\n",
    "df[\"cons_tot_lead\"] = df.groupby(\"hid\")[\"cons_tot\"].shift(-1)\n",
    "df[\"cons_dbt_lead\"] = df.groupby(\"hid\")[\"cons_dbt\"].shift(-1)\n",
    "df[\"net_cons_lead\"] = df[\"cons_tot_lead\"] - df[\"cons_dbt_lead\"]\n",
    "\n",
    "for col in [\"cons_tot_lead\", \"cons_dbt_lead\", \"net_cons_lead\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# net consumption must be positive for log\n",
    "df.loc[df[\"net_cons_lead\"] <= 0, \"net_cons_lead\"] = np.nan\n",
    "\n",
    "df[\"lc\"] = np.where(df[\"wave\"] == 9, np.log(df[\"net_cons_lead\"]), np.nan)\n",
    "\n",
    "# Drop extreme lc\n",
    "mask_w9 = df[\"wave\"] == 9\n",
    "df.loc[mask_w9 & (df[\"lc\"] < 5), \"lc\"] = np.nan\n",
    "df.loc[mask_w9 & (df[\"lc\"] > 10), \"lc\"] = np.nan\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Wave 9 subset and initi_* variables\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "df_w9 = df[df[\"wave\"] == 9].copy()\n",
    "\n",
    "# initi_ egrea_imean / egrea_istd\n",
    "df_w9[\"_initi_egrea_imean\"] = df_w9[\"egrea_imean_bt\"]\n",
    "df_w9[\"initi_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initi_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initi_egrea_istd\"] = df_w9[\"egrea_istd_bt\"]\n",
    "df_w9[\"initi_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initi_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "# Interactions for egrea_imean\n",
    "df_w9[\"_initXi2_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat2A\"]\n",
    "df_w9[\"initXi2_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi2_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi3_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat3A\"]\n",
    "df_w9[\"initXi3_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi3_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi4_egrea_imean\"] = df_w9[\"egrea_imean_bt\"] * df_w9[\"treat4A\"]\n",
    "df_w9[\"initXi4_egrea_imean\"] = df_w9.groupby(\"hid\")[\"_initXi4_egrea_imean\"].transform(\"mean\")\n",
    "\n",
    "# Interactions for egrea_istd\n",
    "df_w9[\"_initXi2_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat2A\"]\n",
    "df_w9[\"initXi2_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi2_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi3_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat3A\"]\n",
    "df_w9[\"initXi3_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi3_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "df_w9[\"_initXi4_egrea_istd\"] = df_w9[\"egrea_istd_bt\"] * df_w9[\"treat4A\"]\n",
    "df_w9[\"initXi4_egrea_istd\"] = df_w9.groupby(\"hid\")[\"_initXi4_egrea_istd\"].transform(\"mean\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Huber robust first stage (approx rreg2 logic)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "sqrtw = np.sqrt(df_w9[\"wgt\"])\n",
    "\n",
    "Y1 = df_w9[\"egrea_imean_pt\"] * sqrtw\n",
    "Y2 = df_w9[\"egrea_istd_pt\"] * sqrtw\n",
    "\n",
    "X1 = df_w9[\"initi_egrea_imean\"] * sqrtw\n",
    "X2 = df_w9[\"initi_egrea_istd\"] * sqrtw\n",
    "\n",
    "M1 = df_w9[\"treat2A\"] * sqrtw\n",
    "M2 = df_w9[\"treat3A\"] * sqrtw\n",
    "M3 = df_w9[\"treat4A\"] * sqrtw\n",
    "\n",
    "Z1 = df_w9[\"initXi2_egrea_imean\"] * sqrtw\n",
    "Z2 = df_w9[\"initXi3_egrea_imean\"] * sqrtw\n",
    "Z3 = df_w9[\"initXi2_egrea_istd\"] * sqrtw\n",
    "Z4 = df_w9[\"initXi3_egrea_istd\"] * sqrtw\n",
    "Z5 = df_w9[\"initXi4_egrea_imean\"] * sqrtw\n",
    "Z6 = df_w9[\"initXi4_egrea_istd\"] * sqrtw\n",
    "\n",
    "X_h = pd.DataFrame({\n",
    "    \"const\": 1.0,\n",
    "    \"X1\": X1,\n",
    "    \"X2\": X2,\n",
    "    \"W\": sqrtw,\n",
    "    \"M1\": M1,\n",
    "    \"M2\": M2,\n",
    "    \"M3\": M3,\n",
    "    \"Z1\": Z1,\n",
    "    \"Z2\": Z2,\n",
    "    \"Z3\": Z3,\n",
    "    \"Z4\": Z4,\n",
    "    \"Z5\": Z5,\n",
    "    \"Z6\": Z6,\n",
    "})\n",
    "\n",
    "# Add country dummies (all; redundancy handled later)\n",
    "for c in [\"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]:\n",
    "    if c in df_w9.columns:\n",
    "        df_w9[c] = pd.to_numeric(df_w9[c], errors=\"coerce\")\n",
    "        X_h[c] = df_w9[c] * sqrtw\n",
    "\n",
    "# Clean finite rows\n",
    "mask_finite = np.isfinite(Y1) & np.isfinite(Y2)\n",
    "for col in X_h.columns:\n",
    "    mask_finite &= np.isfinite(X_h[col])\n",
    "\n",
    "X_h_clean = X_h.loc[mask_finite]\n",
    "Y1_clean = Y1.loc[mask_finite]\n",
    "Y2_clean = Y2.loc[mask_finite]\n",
    "\n",
    "# RLM with Huber T for both equations\n",
    "rlm1 = sm.RLM(Y1_clean, X_h_clean, M=HuberT())\n",
    "res1 = rlm1.fit()\n",
    "\n",
    "rlm2 = sm.RLM(Y2_clean, X_h_clean, M=HuberT())\n",
    "res2 = rlm2.fit()\n",
    "\n",
    "# Extract weights back into full df_w9 index\n",
    "w1 = pd.Series(1.0, index=df_w9.index)\n",
    "w2 = pd.Series(1.0, index=df_w9.index)\n",
    "w1.loc[mask_finite] = res1.weights\n",
    "w2.loc[mask_finite] = res2.weights\n",
    "\n",
    "df_w9[\"_wgt1\"] = w1\n",
    "df_w9[\"_wgt2\"] = w2\n",
    "df_w9[\"huber_wgt\"] = np.sqrt(df_w9[\"_wgt1\"]) * np.sqrt(df_w9[\"_wgt2\"]) * df_w9[\"wgt\"]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 7) Build 2SLS design\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "spec0_vars = [\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"age2\", \"hhsize\",\n",
    "    \"lhhnetinc\", \"liquid\", \"male\", \"k1010_3\",\n",
    "    \"pr2010D\", \"pr2110D\"\n",
    "]\n",
    "cnt_vars = [\"cnt1\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]  # cnt2 omitted as base\n",
    "\n",
    "needed_for_reg = (\n",
    "    [\"lc\", \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "     \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "     \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "     \"huber_wgt\", \"hid\"]\n",
    "    + spec0_vars\n",
    "    + cnt_vars\n",
    "    + [\n",
    "        \"initXi2_egrea_imean\", \"initXi3_egrea_imean\", \"initXi4_egrea_imean\",\n",
    "        \"initXi2_egrea_istd\", \"initXi3_egrea_istd\", \"initXi4_egrea_istd\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for v in needed_for_reg:\n",
    "    if v in df_w9.columns:\n",
    "        df_w9[v] = pd.to_numeric(df_w9[v], errors=\"coerce\")\n",
    "\n",
    "df_reg = df_w9.dropna(subset=[\n",
    "    \"lc\", \"egrea_imean_pt\", \"egrea_istd_pt\",\n",
    "    \"initi_egrea_imean\", \"initi_egrea_istd\",\n",
    "    \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "    \"huber_wgt\"\n",
    "] + spec0_vars)\n",
    "\n",
    "# Scale lc by 100 (as in Stata normalize step)\n",
    "df_reg = df_reg.copy()\n",
    "df_reg[\"lc_scaled\"] = df_reg[\"lc\"] * 100.0\n",
    "\n",
    "# Endogenous\n",
    "endog = df_reg[[\"egrea_imean_pt\", \"egrea_istd_pt\"]]\n",
    "\n",
    "# Exogenous (NO treat dummies here; those are only instruments)\n",
    "exog = pd.DataFrame({\"const\": 1.0}, index=df_reg.index)\n",
    "for v in spec0_vars:\n",
    "    exog[v] = df_reg[v]\n",
    "exog[\"initi_egrea_imean\"] = df_reg[\"initi_egrea_imean\"]\n",
    "exog[\"initi_egrea_istd\"] = df_reg[\"initi_egrea_istd\"]\n",
    "for v in cnt_vars:\n",
    "    if v in df_reg.columns:\n",
    "        exog[v] = df_reg[v]\n",
    "\n",
    "# >>> interaction term between prior uncertainty and treatment 3 (treat3)\n",
    "# Using initXi3_egrea_istd = egrea_istd_bt * treat3A averaged by ID\n",
    "exog[\"prior_uncert_treat3\"] = df_reg[\"initXi3_egrea_istd\"]\n",
    "\n",
    "# Instruments – treat dummies + interactions\n",
    "instr = pd.DataFrame(index=df_reg.index)\n",
    "instr[\"initXi2_egrea_imean\"] = df_reg[\"initXi2_egrea_imean\"]\n",
    "instr[\"initXi3_egrea_imean\"] = df_reg[\"initXi3_egrea_imean\"]\n",
    "instr[\"initXi4_egrea_imean\"] = df_reg[\"initXi4_egrea_imean\"]\n",
    "instr[\"initXi2_egrea_istd\"] = df_reg[\"initXi2_egrea_istd\"]\n",
    "# NOTE: do NOT include initXi3_egrea_istd here, it's in exog\n",
    "instr[\"initXi4_egrea_istd\"] = df_reg[\"initXi4_egrea_istd\"]\n",
    "instr[\"treat2A\"] = df_reg[\"treat2A\"]\n",
    "instr[\"treat3A\"] = df_reg[\"treat3A\"]\n",
    "instr[\"treat4A\"] = df_reg[\"treat4A\"]\n",
    "\n",
    "# Drop constant columns from exog and instr to avoid rank deficiency\n",
    "exog = drop_constant_columns(exog, label=\"exog\")\n",
    "instr = drop_constant_columns(instr, label=\"instr\")\n",
    "\n",
    "# Dep var, weights, clusters\n",
    "y = df_reg[\"lc_scaled\"]\n",
    "weights = df_reg[\"huber_wgt\"]\n",
    "clusters = df_reg[\"hid\"]\n",
    "\n",
    "# Quick diagnostic: matrix rank\n",
    "Z = pd.concat([exog, instr], axis=1)\n",
    "rank_Z = np.linalg.matrix_rank(Z.values)\n",
    "print(\"Combined [exog | instr] columns:\", Z.shape[1], \"  rank:\", rank_Z)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) Run IV 2SLS with Huber weights & clustered SE\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "iv_mod = IV2SLS(\n",
    "    dependent=y,\n",
    "    exog=exog,\n",
    "    endog=endog,\n",
    "    instruments=instr,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "res = iv_mod.fit(\n",
    "    cov_type=\"clustered\",\n",
    "    clusters=clusters\n",
    ")\n",
    "\n",
    "print(res.summary)\n",
    "\n",
    "# Key coefficients on post beliefs\n",
    "beta_mean = res.params[\"egrea_imean_pt\"]\n",
    "se_mean = res.std_errors[\"egrea_imean_pt\"]\n",
    "\n",
    "beta_unc = res.params[\"egrea_istd_pt\"]\n",
    "se_unc = res.std_errors[\"egrea_istd_pt\"]\n",
    "\n",
    "print(\"\\n--- Key coefficients (DV: 100 * log net consumption) ---\")\n",
    "print(f\"First_Moment_Post (egrea_imean_pt):  beta = {beta_mean:.4f},  se = {se_mean:.4f}\")\n",
    "print(f\"Second_Moment_Post (egrea_istd_pt):  beta = {beta_unc:.4f},  se = {se_unc:.4f}\")\n",
    "\n",
    "# Interaction: prior uncertainty × Treatment 3 (initXi3_egrea_istd)\n",
    "beta_inter = res.params[\"prior_uncert_treat3\"]\n",
    "se_inter = res.std_errors[\"prior_uncert_treat3\"]\n",
    "\n",
    "print(\"\\n--- Interaction: prior uncertainty × Treatment 3 (initXi3_egrea_istd) ---\")\n",
    "print(f\"beta = {beta_inter:.4f},  se = {se_inter:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477afa2c",
   "metadata": {},
   "source": [
    "### import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Helper: run Table-2 style regression for a given moment\n",
    "#   var = \"egrea_imean\"  (mean expectations)\n",
    "#   var = \"egrea_istd\"   (expected uncertainty)\n",
    "# Returns a dict with coeffs + SEs for:\n",
    "#   Prior, Treat2×Prior (EA GDP 2nd m), Treat2 indicator\n",
    "# ---------------------------------------------------------\n",
    "def run_table2_column(df_all, var):\n",
    "    prior_col = f\"{var}_bt\"\n",
    "    post_col  = f\"{var}_pt\"\n",
    "\n",
    "    # Work only with wave 9 (same as Stata do-file)\n",
    "    df = df_all[df_all[\"wave\"] == 9].copy()\n",
    "\n",
    "    # Make sure required vars are numeric\n",
    "    needed = [\n",
    "        \"wgt\", prior_col, post_col,\n",
    "        \"treat1\", \"treat2\", \"treat3\", \"treat4\", \"treat5\",\n",
    "        \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"\n",
    "    ]\n",
    "    for v in needed:\n",
    "        if v in df.columns:\n",
    "            df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "\n",
    "    # Treatment indicators as in the do-file\n",
    "    df[\"treat2A\"] = (df[\"treat2\"] == 1).astype(float)\n",
    "    df[\"treat3A\"] = (df[\"treat3\"] == 1).astype(float)  # EA GDP – 2nd m\n",
    "    df[\"treat4A\"] = (df[\"treat4\"] == 1).astype(float)\n",
    "    df[\"treat5A\"] = (df[\"treat5\"] == 1).astype(float)\n",
    "\n",
    "    # Prior and interactions (initi_* and initXi*_* in Stata)\n",
    "    df[f\"initi_{var}\"]      = df[prior_col]\n",
    "    df[f\"initXi2_{var}\"]    = df[prior_col] * df[\"treat2A\"]\n",
    "    df[f\"initXi3_{var}\"]    = df[prior_col] * df[\"treat3A\"]  # <-- Treatment 2 × Prior in the paper\n",
    "    df[f\"initXi4_{var}\"]    = df[prior_col] * df[\"treat4A\"]\n",
    "    df[f\"initXi5_{var}\"]    = df[prior_col] * df[\"treat5A\"]\n",
    "\n",
    "    # Keep rows with all needed data\n",
    "    cols_for_rreg = [\n",
    "        post_col, f\"initi_{var}\",\n",
    "        f\"initXi2_{var}\", f\"initXi3_{var}\", f\"initXi4_{var}\", f\"initXi5_{var}\",\n",
    "        \"treat2A\", \"treat3A\", \"treat4A\", \"treat5A\",\n",
    "        \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "        \"wgt\"\n",
    "    ]\n",
    "    d = df[cols_for_rreg].dropna().copy()\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Step 1: approximate rreg2 to get Huber weights\n",
    "    #   rreg2 _Y1 _X1 _W _M1 ... _cnt*   (Stata code)\n",
    "    # -----------------------------------------------------\n",
    "    sqrtw = np.sqrt(d[\"wgt\"])\n",
    "\n",
    "    Y = d[post_col] * sqrtw\n",
    "    X = pd.DataFrame({\n",
    "        \"const\": 1.0,\n",
    "        \"prior\": d[f\"initi_{var}\"] * sqrtw,\n",
    "        \"W\": sqrtw,\n",
    "        \"M2\": d[\"treat2A\"] * sqrtw,\n",
    "        \"M3\": d[\"treat3A\"] * sqrtw,\n",
    "        \"M4\": d[\"treat4A\"] * sqrtw,\n",
    "        \"M5\": d[\"treat5A\"] * sqrtw,\n",
    "        \"Z2\": d[f\"initXi2_{var}\"] * sqrtw,\n",
    "        \"Z3\": d[f\"initXi3_{var}\"] * sqrtw,\n",
    "        \"Z4\": d[f\"initXi4_{var}\"] * sqrtw,\n",
    "        \"Z5\": d[f\"initXi5_{var}\"] * sqrtw,\n",
    "    })\n",
    "\n",
    "    for c in [\"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\"]:\n",
    "        X[c] = d[c] * sqrtw\n",
    "\n",
    "    # Drop rows with any inf/NaN in the RLM step\n",
    "    mask = np.isfinite(Y)\n",
    "    for col in X.columns:\n",
    "        mask &= np.isfinite(X[col])\n",
    "\n",
    "    Y_c = Y[mask]\n",
    "    X_c = X.loc[mask]\n",
    "\n",
    "    rlm = sm.RLM(Y_c, X_c, M=HuberT())\n",
    "    rlm_res = rlm.fit()\n",
    "\n",
    "    # Huber weights × original sampling weights (Stata: _wgt1 * wgt)\n",
    "    huber_aux = pd.Series(1.0, index=d.index)\n",
    "    huber_aux.loc[mask.index[mask]] = rlm_res.weights\n",
    "    d[\"w_final\"] = huber_aux * d[\"wgt\"]\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # Step 2: weighted OLS with robust SE (Table 2 spec)\n",
    "    #   var_pt on Prior, I{Tj}×Prior, Tj dummies, countries\n",
    "    # -----------------------------------------------------\n",
    "    X2 = pd.DataFrame({\n",
    "        \"const\": 1.0,\n",
    "        \"Prior\": d[f\"initi_{var}\"],\n",
    "        \"T1xPrior\": d[f\"initXi2_{var}\"],  # EA GDP – 1st m\n",
    "        \"T2xPrior\": d[f\"initXi3_{var}\"],  # EA GDP – 2nd m (the one you care about)\n",
    "        \"T3xPrior\": d[f\"initXi4_{var}\"],  # EA GDP – 1st & 2nd m\n",
    "        \"T4xPrior\": d[f\"initXi5_{var}\"],  # Country GDP – 2nd m\n",
    "        \"Treat1\": d[\"treat2A\"],\n",
    "        \"Treat2\": d[\"treat3A\"],          # EA GDP – 2nd m indicator\n",
    "        \"Treat3\": d[\"treat4A\"],\n",
    "        \"Treat4\": d[\"treat5A\"],\n",
    "    })\n",
    "\n",
    "    # Country dummies: drop one (cnt2) to avoid collinearity\n",
    "    X2[\"cnt1\"] = d[\"cnt1\"]\n",
    "    X2[\"cnt3\"] = d[\"cnt3\"]\n",
    "    X2[\"cnt4\"] = d[\"cnt4\"]\n",
    "    X2[\"cnt5\"] = d[\"cnt5\"]\n",
    "    X2[\"cnt6\"] = d[\"cnt6\"]\n",
    "\n",
    "    wls = sm.WLS(d[post_col], X2, weights=d[\"w_final\"])\n",
    "    wls_res = wls.fit(cov_type=\"HC1\")\n",
    "\n",
    "    out = {\n",
    "        \"N\": int(wls_res.nobs),\n",
    "        \"Prior_beta\":      wls_res.params[\"Prior\"],\n",
    "        \"Prior_se\":        wls_res.bse[\"Prior\"],\n",
    "        \"T2xPrior_beta\":   wls_res.params[\"T2xPrior\"],\n",
    "        \"T2xPrior_se\":     wls_res.bse[\"T2xPrior\"],\n",
    "        \"Treat2_beta\":     wls_res.params[\"Treat2\"],\n",
    "        \"Treat2_se\":       wls_res.bse[\"Treat2\"],\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main: load data and run both columns\n",
    "# ---------------------------------------------------------\n",
    "df_all = pd.read_stata(\"ces_gdp_rct_v1a.dta\", convert_categoricals=False)\n",
    "\n",
    "col1 = run_table2_column(df_all, \"egrea_imean\")  # Mean expectations\n",
    "col2 = run_table2_column(df_all, \"egrea_istd\")   # Expected uncertainty\n",
    "\n",
    "print(\"=== Table 2, Column (1): Mean expectations ===\")\n",
    "print(f\"N obs = {col1['N']}\")\n",
    "print(f\"Prior:                beta = {col1['Prior_beta']:.3f},  se = {col1['Prior_se']:.3f}\")\n",
    "print(f\"I{{Treatment 2}}×Prior: beta = {col1['T2xPrior_beta']:.3f},  se = {col1['T2xPrior_se']:.3f}\")\n",
    "print(f\"Treatment 2 indicator: beta = {col1['Treat2_beta']:.3f},  se = {col1['Treat2_se']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Table 2, Column (2): Expected uncertainty ===\")\n",
    "print(f\"N obs = {col2['N']}\")\n",
    "print(f\"Prior:                beta = {col2['Prior_beta']:.3f},  se = {col2['Prior_se']:.3f}\")\n",
    "print(f\"I{{Treatment 2}}×Prior: beta = {col2['T2xPrior_beta']:.3f},  se = {col2['T2xPrior_se']:.3f}\")\n",
    "print(f\"Treatment 2 indicator: beta = {col2['Treat2_beta']:.3f},  se = {col2['Treat2_se']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a524be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ces_reduced_second_moment_consumption_w10_w13.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Load CES data\n",
    "# -------------------------------------------------------\n",
    "df = pd.read_stata(\"ces_gdp_rct_v1a.dta\", convert_categoricals=False)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Basic restrictions and numeric coercion\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Drop treatment 5 (country GDP 2nd moment)\n",
    "if \"treat5\" in df.columns:\n",
    "    df = df[df[\"treat5\"] != 1]\n",
    "\n",
    "# Drop speeders if available\n",
    "if \"flag_speeder\" in df.columns:\n",
    "    df = df[df[\"flag_speeder\"] != 1]\n",
    "\n",
    "# Keep only waves 7–16 (as in prep do-file)\n",
    "df = df[df[\"wave\"].between(7, 16)]\n",
    "\n",
    "# Variables we want to be sure are numeric\n",
    "num_vars = [\n",
    "    \"wave\", \"hid\", \"wgt\",\n",
    "    \"cons_tot\", \"cons_dbt\",\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"hhsize\", \"lhhnetinc\",\n",
    "    \"liquid\", \"male\", \"k1010_3\", \"pr2010\", \"pr2110\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\"\n",
    "]\n",
    "for v in num_vars:\n",
    "    if v in df.columns:\n",
    "        df[v] = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Treatment dummies and controls (same as before)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "df[\"treat2A\"] = np.where(df[\"treat2\"] == 1, 1.0, 0.0)\n",
    "df[\"treat3A\"] = np.where(df[\"treat3\"] == 1, 1.0, 0.0)\n",
    "df[\"treat4A\"] = np.where(df[\"treat4\"] == 1, 1.0, 0.0)\n",
    "\n",
    "df[\"age2\"] = (df[\"age\"] ** 2) / 100.0\n",
    "df[\"pr2010D\"] = np.where(df[\"pr2010\"] == 2, 1.0, 0.0)\n",
    "df[\"pr2110D\"] = np.where(df[\"pr2110\"] == 2, 1.0, 0.0)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) Net consumption and log(consumption) for all waves\n",
    "#    (same construction as for wave 10, applied also to 13)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "df[\"net_cons\"] = df[\"cons_tot\"] - df[\"cons_dbt\"]\n",
    "df.loc[df[\"net_cons\"] <= 0, \"net_cons\"] = np.nan  # cannot log non-positive\n",
    "\n",
    "df[\"lc_all\"] = np.log(df[\"net_cons\"])\n",
    "# Trim extremes as in the Stata code (5–10)\n",
    "df.loc[(df[\"lc_all\"] < 5) | (df[\"lc_all\"] > 10), \"lc_all\"] = np.nan\n",
    "\n",
    "# Extract wave-specific consumption (level + log)\n",
    "cons10 = df[df[\"wave\"] == 10][[\"hid\", \"net_cons\", \"lc_all\"]].rename(\n",
    "    columns={\"net_cons\": \"net_cons_w10\", \"lc_all\": \"lc_w10\"}\n",
    ")\n",
    "cons13 = df[df[\"wave\"] == 13][[\"hid\", \"net_cons\", \"lc_all\"]].rename(\n",
    "    columns={\"net_cons\": \"net_cons_w13\", \"lc_all\": \"lc_w13\"}\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) Baseline wave-9 data (beliefs + treatments + controls)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "df9 = df[df[\"wave\"] == 9].copy()\n",
    "\n",
    "# Make sure these are numeric in the wave-9 slice as well\n",
    "vars_wave9_numeric = [\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"age2\", \"hhsize\",\n",
    "    \"lhhnetinc\", \"liquid\", \"male\", \"k1010_3\",\n",
    "    \"pr2010D\", \"pr2110D\", \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\",\n",
    "    \"treat2A\", \"treat3A\", \"treat4A\", \"wgt\"\n",
    "]\n",
    "for v in vars_wave9_numeric:\n",
    "    if v in df9.columns:\n",
    "        df9[v] = pd.to_numeric(df9[v], errors=\"coerce\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Merge in wave-10 and wave-13 consumption\n",
    "# -------------------------------------------------------\n",
    "\n",
    "df9 = df9.merge(cons10, on=\"hid\", how=\"left\")\n",
    "df9 = df9.merge(cons13, on=\"hid\", how=\"left\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Build reduced dataset and save as CSV\n",
    "# -------------------------------------------------------\n",
    "\n",
    "keep_cols = [\n",
    "    \"hid\", \"wgt\",\n",
    "    # treatments\n",
    "    \"treat1\", \"treat2\", \"treat3\", \"treat4\",\n",
    "    \"treat2A\", \"treat3A\", \"treat4A\",\n",
    "    # beliefs: prior and posterior (mean & uncertainty)\n",
    "    \"egrea_imean_bt\", \"egrea_imean_pt\",\n",
    "    \"egrea_istd_bt\", \"egrea_istd_pt\",\n",
    "    # controls\n",
    "    \"dedu2\", \"dedu3\", \"age\", \"age2\", \"hhsize\",\n",
    "    \"lhhnetinc\", \"liquid\", \"male\", \"k1010_3\",\n",
    "    \"pr2010D\", \"pr2110D\",\n",
    "    \"cnt1\", \"cnt2\", \"cnt3\", \"cnt4\", \"cnt5\", \"cnt6\",\n",
    "    # consumption outcomes\n",
    "    \"net_cons_w10\", \"lc_w10\",\n",
    "    \"net_cons_w13\", \"lc_w13\",\n",
    "]\n",
    "\n",
    "# Keep only columns that actually exist, in case some are missing\n",
    "keep_cols = [c for c in keep_cols if c in df9.columns]\n",
    "\n",
    "reduced = df9[keep_cols].copy()\n",
    "\n",
    "# Save to CSV (change path if you want it somewhere specific)\n",
    "reduced.to_csv(\"ces_reduced_second_moment_consumption_w10_w13.csv\", index=False)\n",
    "print(\"Saved:\", \"ces_reduced_second_moment_consumption_w10_w13.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27d48479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CES_RF_SMT.csv saved.\n",
      "Shape: (4104, 29)\n",
      "Columns: ['hid', 'wgt', 'Control_Group', 'Second_Moment_Treatment', 'First_Moment_Expectation_Prior', 'First_Moment_Expectation_Post', 'Second_Moment_Prior', 'Second_Moment_Post', 'Highschool_Educated', 'Tertiary_Educated', 'Age', 'age2', 'Household_Size', 'Log_Household_Net_Income', 'liquid', 'Male', 'Growth_Uncertainty_Probability', 'pr2010D', 'pr2110D', 'Belgian', 'Danish', 'Spanish', 'French', 'Italian', 'Dutch', 'net_cons_w10', 'lc_w10', 'net_cons_w13', 'lc_w13']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load the reduced dataset\n",
    "# --------------------------------------------------\n",
    "df = pd.read_csv(\"ces_reduced_second_moment_consumption_w10_w13.csv\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Keep only control & 2nd-moment treatment\n",
    "#    (treat1 == 1 or treat3A == 1)\n",
    "# --------------------------------------------------\n",
    "mask = (df[\"treat1\"] == 1) | (df[\"treat3A\"] == 1)\n",
    "CES_RF_SMT = df.loc[mask].copy()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Drop unneeded treatment variables\n",
    "# --------------------------------------------------\n",
    "cols_to_drop = [c for c in [\"treat2\", \"treat3\", \"treat4\", \"treat5\", \"treat2A\", \"treat4A\"] if c in CES_RF_SMT.columns]\n",
    "CES_RF_SMT = CES_RF_SMT.drop(columns=cols_to_drop)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Rename variables with your conventions\n",
    "# --------------------------------------------------\n",
    "rename_dict = {\n",
    "    # Treatments\n",
    "    \"treat1\": \"Control_Group\",\n",
    "    \"treat3A\": \"Second_Moment_Treatment\",\n",
    "\n",
    "    # Education / demographics / controls\n",
    "    \"dedu2\": \"Highschool_Educated\",\n",
    "    \"dedu3\": \"Tertiary_Educated\",\n",
    "    \"age\": \"Age\",\n",
    "    \"hhsize\": \"Household_Size\",\n",
    "    \"male\": \"Male\",\n",
    "    \"lhhnetinc\": \"Log_Household_Net_Income\",\n",
    "    \"k1010_3\": \"Growth_Uncertainty_Probability\",\n",
    "\n",
    "    # Countries\n",
    "    \"cnt1\": \"Belgian\",\n",
    "    \"cnt2\": \"Danish\",\n",
    "    \"cnt3\": \"Spanish\",\n",
    "    \"cnt4\": \"French\",\n",
    "    \"cnt5\": \"Italian\",\n",
    "    \"cnt6\": \"Dutch\",\n",
    "\n",
    "    # Expectations (moments)\n",
    "    \"egrea_imean_bt\": \"First_Moment_Expectation_Prior\",\n",
    "    \"egrea_istd_bt\": \"Second_Moment_Prior\",\n",
    "    \"egrea_imean_pt\": \"First_Moment_Expectation_Post\",\n",
    "    \"egrea_istd_pt\": \"Second_Moment_Post\",\n",
    "\n",
    "    # Survey-type dummies (if they exist with these names)\n",
    "    \"pr2010_Non-probabilistic\": \"Planned_Spending_Consumer_Goods\",\n",
    "    \"pr2110_CAWI\": \"Planned_Spending_Durable_Goods\",\n",
    "\n",
    "    # Consumption totals\n",
    "    \"cons_tot_wave10\": \"Total_Consumption_Oct_2020\",\n",
    "    \"cons_tot_wave13\": \"Total_Consumption_Jan_2021\",\n",
    "    \"cons_tot_wave7\": \"Total_Consumption_Prior\",\n",
    "\n",
    "    # Debt totals\n",
    "    \"cons_dbt_wave10\": \"Total_Debt_Oct_2020\",\n",
    "    \"cons_dbt_wave13\": \"Total_Debt_Jan_2021\",\n",
    "    \"cons_dbt_wave7\": \"Total_Debt_Prior\",\n",
    "\n",
    "    # Weights\n",
    "    \"wgt_wave10\": \"Weight_Oct\",\n",
    "    \"wgt_wave13\": \"Weight_Jan\",\n",
    "    \"wgt_wave7\": \"Weight_Prior\",\n",
    "\n",
    "    # Liquidity by wave\n",
    "    \"liquid_wave10\": \"Liquidity_Oct\",\n",
    "    \"liquid_wave13\": \"Liquidity_Jan\",\n",
    "    \"liquid_wave7\": \"Liquidity_Prior\",\n",
    "\n",
    "    # Portfolio shares at baseline (wave 9)\n",
    "    \"sh_sav_wave9\": \"Savings_Prior\",\n",
    "    \"sh_stock_wave9\": \"Stocks_Prior\",\n",
    "    \"sh_mutf_wave9\": \"Mutual_Funds_Prior\",\n",
    "    \"sh_bond_wave9\": \"Bonds_Prior\",\n",
    "}\n",
    "\n",
    "# Only rename columns that actually exist\n",
    "CES_RF_SMT = CES_RF_SMT.rename(columns={k: v for k, v in rename_dict.items() if k in CES_RF_SMT.columns})\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Save final subset\n",
    "# --------------------------------------------------\n",
    "CES_RF_SMT.to_csv(\"CES_RF_SMT.csv\", index=False)\n",
    "\n",
    "print(\"CES_RF_SMT.csv saved.\")\n",
    "print(\"Shape:\", CES_RF_SMT.shape)\n",
    "print(\"Columns:\", CES_RF_SMT.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8f234d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OLS (Oct 2020, lc_w10) on Second_Moment_Treatment + controls ===\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                              5.6485      0.181     31.123      0.000       5.293       6.004\n",
      "Second_Moment_Treatment            0.0194      0.023      0.848      0.397      -0.025       0.064\n",
      "Highschool_Educated               -0.0075      0.040     -0.190      0.850      -0.085       0.070\n",
      "Tertiary_Educated                  0.1299      0.036      3.584      0.000       0.059       0.201\n",
      "Age                                0.0165      0.005      3.492      0.000       0.007       0.026\n",
      "Age_Squared                       -0.0130      0.005     -2.708      0.007      -0.022      -0.004\n",
      "Household_Size                     0.1088      0.010     11.005      0.000       0.089       0.128\n",
      "Log_Household_Net_Income           0.0882      0.015      6.017      0.000       0.059       0.117\n",
      "Male                               0.0363      0.024      1.513      0.130      -0.011       0.083\n",
      "Growth_Uncertainty_Probability     0.0464      0.048      0.970      0.332      -0.047       0.140\n",
      "pr2010D                           -0.1950      0.030     -6.569      0.000      -0.253      -0.137\n",
      "pr2110D                            0.0025      0.034      0.073      0.942      -0.064       0.069\n",
      "First_Moment_Expectation_Prior    -0.0015      0.001     -1.382      0.167      -0.004       0.001\n",
      "Second_Moment_Prior                0.0016      0.005      0.294      0.769      -0.009       0.012\n",
      "Belgian                           -0.1872      0.047     -4.009      0.000      -0.279      -0.096\n",
      "Spanish                           -0.3866      0.037    -10.446      0.000      -0.459      -0.314\n",
      "French                            -0.2006      0.035     -5.802      0.000      -0.268      -0.133\n",
      "Italian                           -0.2297      0.037     -6.236      0.000      -0.302      -0.157\n",
      "Dutch                             -0.0042      0.043     -0.100      0.920      -0.088       0.079\n",
      "liquid                             0.1727      0.029      5.903      0.000       0.115       0.230\n",
      "==================================================================================================\n",
      "\n",
      "Oct 2020 reduced form: beta = 0.0194, se = 0.0229\n",
      "\n",
      "=== OLS (Jan 2021, lc_w13) on Second_Moment_Treatment + controls ===\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                              5.5429      0.180     30.763      0.000       5.190       5.896\n",
      "Second_Moment_Treatment            0.0409      0.025      1.667      0.096      -0.007       0.089\n",
      "Highschool_Educated                0.0011      0.040      0.027      0.978      -0.077       0.079\n",
      "Tertiary_Educated                  0.1417      0.035      4.021      0.000       0.073       0.211\n",
      "Age                                0.0231      0.005      4.559      0.000       0.013       0.033\n",
      "Age_Squared                       -0.0193      0.005     -3.769      0.000      -0.029      -0.009\n",
      "Household_Size                     0.0984      0.010     10.028      0.000       0.079       0.118\n",
      "Log_Household_Net_Income           0.0862      0.014      6.090      0.000       0.058       0.114\n",
      "Male                               0.0447      0.025      1.805      0.071      -0.004       0.093\n",
      "Growth_Uncertainty_Probability     0.0579      0.048      1.205      0.228      -0.036       0.152\n",
      "pr2010D                           -0.2182      0.030     -7.263      0.000      -0.277      -0.159\n",
      "pr2110D                            0.0034      0.037      0.091      0.927      -0.070       0.077\n",
      "First_Moment_Expectation_Prior     0.0012      0.001      1.049      0.294      -0.001       0.004\n",
      "Second_Moment_Prior                0.0055      0.006      0.935      0.350      -0.006       0.017\n",
      "Belgian                           -0.1916      0.047     -4.046      0.000      -0.284      -0.099\n",
      "Spanish                           -0.3329      0.041     -8.174      0.000      -0.413      -0.253\n",
      "French                            -0.2173      0.035     -6.281      0.000      -0.285      -0.149\n",
      "Italian                           -0.2561      0.040     -6.460      0.000      -0.334      -0.178\n",
      "Dutch                              0.0134      0.047      0.288      0.773      -0.078       0.105\n",
      "liquid                             0.1046      0.028      3.796      0.000       0.051       0.159\n",
      "==================================================================================================\n",
      "\n",
      "Jan 2021 reduced form: beta = 0.0409, se = 0.0245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1) Load reduced-form dataset\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(\"CES_RF_SMT.csv\")\n",
    "\n",
    "# We only want control vs second-moment treatment\n",
    "# (this should already be true, but just to enforce)\n",
    "df = df[(df[\"Control_Group\"] == 1) | (df[\"Second_Moment_Treatment\"] == 1)]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2) Basic cleaning and construction of controls\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Make sure key vars are numeric\n",
    "num_cols = [\n",
    "    \"lc_w10\", \"lc_w13\",\n",
    "    \"Second_Moment_Treatment\",\n",
    "    \"Highschool_Educated\", \"Tertiary_Educated\",\n",
    "    \"Age\", \"Household_Size\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Male\",\n",
    "    \"Growth_Uncertainty_Probability\",\n",
    "    \"pr2010D\",\n",
    "    \"pr2110D\",\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"liquid\",\n",
    "    \"Belgian\", \"Spanish\", \"French\", \"Italian\", \"Dutch\",\n",
    "    \"wgt\"\n",
    "]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Age squared (as in the original Stata specs: age^2/100)\n",
    "df[\"Age_Squared\"] = (df[\"Age\"] ** 2) / 100.0\n",
    "\n",
    "# Common controls across both horizons\n",
    "common_controls = [\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    \"Age\",\n",
    "    \"Age_Squared\",\n",
    "    \"Household_Size\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"Male\",\n",
    "    \"Growth_Uncertainty_Probability\",\n",
    "    \"pr2010D\",\n",
    "    \"pr2110D\",\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    # Country dummies (Danish omitted as the base category)\n",
    "    \"Belgian\", \"Spanish\", \"French\", \"Italian\", \"Dutch\",\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3) Regression 1: lc_w10 on Second_Moment_Treatment\n",
    "# ---------------------------------------------------------\n",
    "reg10_controls = common_controls + [\"liquid\"]\n",
    "\n",
    "df10 = df.dropna(subset=[\"lc_w10\", \"Second_Moment_Treatment\", \"wgt\"] + reg10_controls).copy()\n",
    "\n",
    "y10 = df10[\"lc_w10\"]\n",
    "X10 = df10[[\"Second_Moment_Treatment\"] + reg10_controls]\n",
    "X10 = sm.add_constant(X10)\n",
    "\n",
    "w10 = df10[\"wgt\"]\n",
    "\n",
    "mod10 = sm.WLS(y10, X10, weights=w10)\n",
    "res10 = mod10.fit(cov_type=\"HC1\")  # heteroskedasticity-robust\n",
    "\n",
    "print(\"\\n=== OLS (Oct 2020, lc_w10) on Second_Moment_Treatment + controls ===\")\n",
    "print(res10.summary().tables[1])\n",
    "\n",
    "beta_10 = res10.params[\"Second_Moment_Treatment\"]\n",
    "se_10 = res10.bse[\"Second_Moment_Treatment\"]\n",
    "print(f\"\\nOct 2020 reduced form: beta = {beta_10:.4f}, se = {se_10:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4) Regression 2: lc_w13 on Second_Moment_Treatment\n",
    "# ---------------------------------------------------------\n",
    "reg13_controls = common_controls + [\"liquid\"]\n",
    "\n",
    "df13 = df.dropna(subset=[\"lc_w13\", \"Second_Moment_Treatment\", \"wgt\"] + reg13_controls).copy()\n",
    "\n",
    "y13 = df13[\"lc_w13\"]\n",
    "X13 = df13[[\"Second_Moment_Treatment\"] + reg13_controls]\n",
    "X13 = sm.add_constant(X13)\n",
    "\n",
    "w13 = df13[\"wgt\"]\n",
    "\n",
    "mod13 = sm.WLS(y13, X13, weights=w13)\n",
    "res13 = mod13.fit(cov_type=\"HC1\")\n",
    "\n",
    "print(\"\\n=== OLS (Jan 2021, lc_w13) on Second_Moment_Treatment + controls ===\")\n",
    "print(res13.summary().tables[1])\n",
    "\n",
    "beta_13 = res13.params[\"Second_Moment_Treatment\"]\n",
    "se_13 = res13.bse[\"Second_Moment_Treatment\"]\n",
    "print(f\"\\nJan 2021 reduced form: beta = {beta_13:.4f}, se = {se_13:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffabce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Wave 10: lc_w10 on Second_Moment_Treatment + controls ===\n",
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 lc_w10   R-squared:                       0.200\n",
      "Model:                            WLS   Adj. R-squared:                  0.195\n",
      "Method:                 Least Squares   F-statistic:                     27.18\n",
      "Date:                Tue, 18 Nov 2025   Prob (F-statistic):           1.57e-71\n",
      "Time:                        12:49:42   Log-Likelihood:                -2225.8\n",
      "No. Observations:                2571   AIC:                             4484.\n",
      "Df Residuals:                    2555   BIC:                             4577.\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                              5.9807      0.174     34.380      0.000       5.640       6.322\n",
      "Second_Moment_Treatment            0.0089      0.025      0.358      0.720      -0.040       0.058\n",
      "First_Moment_Expectation_Prior    -0.0014      0.001     -1.128      0.259      -0.004       0.001\n",
      "Second_Moment_Prior             6.025e-05      0.006      0.010      0.992      -0.012       0.012\n",
      "Highschool_Educated                0.0008      0.041      0.019      0.985      -0.080       0.081\n",
      "Tertiary_Educated                  0.1387      0.038      3.655      0.000       0.064       0.213\n",
      "Age                                0.0030      0.001      3.424      0.001       0.001       0.005\n",
      "Household_Size                     0.1168      0.010     11.131      0.000       0.096       0.137\n",
      "Log_Household_Net_Income           0.0858      0.017      5.115      0.000       0.053       0.119\n",
      "liquid                             0.1490      0.032      4.713      0.000       0.087       0.211\n",
      "Male                               0.0416      0.026      1.598      0.110      -0.009       0.093\n",
      "Belgian                           -0.3585      0.047     -7.694      0.000      -0.450      -0.267\n",
      "Spanish                           -0.4028      0.040    -10.018      0.000      -0.482      -0.324\n",
      "French                            -0.1953      0.037     -5.220      0.000      -0.269      -0.122\n",
      "Italian                           -0.2490      0.038     -6.542      0.000      -0.324      -0.174\n",
      "Dutch                             -0.1659      0.040     -4.139      0.000      -0.244      -0.087\n",
      "==============================================================================\n",
      "Omnibus:                      169.521   Durbin-Watson:                   2.014\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              764.425\n",
      "Skew:                           0.094   Prob(JB):                    1.02e-166\n",
      "Kurtosis:                       5.665   Cond. No.                         586.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
      "\n",
      "=== Wave 13: lc_w13 on Second_Moment_Treatment + controls ===\n",
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 lc_w13   R-squared:                       0.166\n",
      "Model:                            WLS   Adj. R-squared:                  0.161\n",
      "Method:                 Least Squares   F-statistic:                     22.76\n",
      "Date:                Tue, 18 Nov 2025   Prob (F-statistic):           2.09e-59\n",
      "Time:                        12:49:42   Log-Likelihood:                -2299.1\n",
      "No. Observations:                2571   AIC:                             4630.\n",
      "Df Residuals:                    2555   BIC:                             4724.\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:                  HC1                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                              5.9087      0.164     36.135      0.000       5.588       6.229\n",
      "Second_Moment_Treatment            0.0381      0.026      1.475      0.140      -0.013       0.089\n",
      "First_Moment_Expectation_Prior     0.0020      0.001      1.481      0.139      -0.001       0.005\n",
      "Second_Moment_Prior                0.0069      0.006      1.077      0.282      -0.006       0.019\n",
      "Highschool_Educated                0.0174      0.043      0.408      0.683      -0.066       0.101\n",
      "Tertiary_Educated                  0.1421      0.038      3.695      0.000       0.067       0.218\n",
      "Age                                0.0039      0.001      4.274      0.000       0.002       0.006\n",
      "Household_Size                     0.1077      0.012      9.345      0.000       0.085       0.130\n",
      "Log_Household_Net_Income           0.0842      0.016      5.216      0.000       0.053       0.116\n",
      "liquid                             0.0926      0.030      3.119      0.002       0.034       0.151\n",
      "Male                               0.0562      0.026      2.165      0.030       0.005       0.107\n",
      "Belgian                           -0.3228      0.045     -7.231      0.000      -0.410      -0.235\n",
      "Spanish                           -0.2937      0.044     -6.677      0.000      -0.380      -0.208\n",
      "French                            -0.1898      0.037     -5.143      0.000      -0.262      -0.117\n",
      "Italian                           -0.2651      0.040     -6.651      0.000      -0.343      -0.187\n",
      "Dutch                             -0.1281      0.039     -3.277      0.001      -0.205      -0.051\n",
      "==============================================================================\n",
      "Omnibus:                      201.321   Durbin-Watson:                   2.021\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1134.407\n",
      "Skew:                          -0.029   Prob(JB):                    4.64e-247\n",
      "Kurtosis:                       6.254   Cond. No.                         586.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load your reduced-form dataset\n",
    "CES_RF_SMT = pd.read_csv(\"CES_RF_SMT.csv\")\n",
    "\n",
    "# 1) Drop any observation with a missing value in *any* column\n",
    "CES_RF_SMT = CES_RF_SMT.dropna(how=\"any\")\n",
    "\n",
    "# 2) Common controls (adjust this list to match your actual column names)\n",
    "control_cols = [\n",
    "    \"First_Moment_Expectation_Prior\",\n",
    "    \"Second_Moment_Prior\",\n",
    "    \"Highschool_Educated\",\n",
    "    \"Tertiary_Educated\",\n",
    "    \"Age\",\n",
    "    \"Household_Size\",\n",
    "    \"Log_Household_Net_Income\",\n",
    "    \"liquid\",          # using the single liquidity variable you said you have\n",
    "    \"Male\",\n",
    "    \"Belgian\", \"Spanish\", \"French\", \"Italian\", \"Dutch\"  # country dummies, omitting base\n",
    "]\n",
    "\n",
    "# 3) Wave 10 regression: lc_w10 on Second_Moment_Treatment + controls\n",
    "y10 = CES_RF_SMT[\"lc_w10\"]\n",
    "X10 = CES_RF_SMT[[\"Second_Moment_Treatment\"] + control_cols].copy()\n",
    "X10 = sm.add_constant(X10)\n",
    "\n",
    "w10 = CES_RF_SMT[\"wgt\"]   # single weight variable\n",
    "\n",
    "mod10 = sm.WLS(y10, X10, weights=w10)\n",
    "res10 = mod10.fit(cov_type=\"HC1\")\n",
    "print(\"\\n=== Wave 10: lc_w10 on Second_Moment_Treatment + controls ===\")\n",
    "print(res10.summary())\n",
    "\n",
    "# 4) Wave 13 regression: lc_w13 on Second_Moment_Treatment + controls\n",
    "y13 = CES_RF_SMT[\"lc_w13\"]\n",
    "X13 = CES_RF_SMT[[\"Second_Moment_Treatment\"] + control_cols].copy()\n",
    "X13 = sm.add_constant(X13)\n",
    "\n",
    "w13 = CES_RF_SMT[\"wgt\"]   # same weights (that’s all we have)\n",
    "\n",
    "mod13 = sm.WLS(y13, X13, weights=w13)\n",
    "res13 = mod13.fit(cov_type=\"HC1\")\n",
    "print(\"\\n=== Wave 13: lc_w13 on Second_Moment_Treatment + controls ===\")\n",
    "print(res13.summary())\n",
    "CES_RF_SMT.to_csv(\"CES_RF_SMT_clean.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb04cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
